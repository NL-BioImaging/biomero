<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Cellprofiler tutorial &mdash; BIOMERO 1 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="biomero" href="modules.html" />
    <link rel="prev" title="BIOMERO - BioImage analysis in OMERO" href="readme_link.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            BIOMERO
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">BIOMERO - Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="readme_link.html">BIOMERO - BioImage analysis in OMERO</a></li>
<li class="toctree-l1"><a class="reference internal" href="readme_link.html#overview">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="readme_link.html#quickstart">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="readme_link.html#prerequisites-getting-started-with-biomero">Prerequisites &amp; Getting Started with BIOMERO</a></li>
<li class="toctree-l1"><a class="reference internal" href="readme_link.html#biomero-scripts">BIOMERO scripts</a></li>
<li class="toctree-l1"><a class="reference internal" href="readme_link.html#docker-containers">(Docker) containers</a></li>
<li class="toctree-l1"><a class="reference internal" href="readme_link.html#see-the-tutorials">See the tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="readme_link.html#ssh">SSH</a></li>
<li class="toctree-l1"><a class="reference internal" href="readme_link.html#slurmclient-class">SlurmClient class</a></li>
<li class="toctree-l1"><a class="reference internal" href="readme_link.html#slurm-config-ini">slurm-config.ini</a></li>
<li class="toctree-l1"><a class="reference internal" href="readme_link.html#how-to-add-an-existing-workflow">How to add an existing workflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="readme_link.html#how-to-add-your-new-custom-workflow">How to add your new custom workflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="readme_link.html#slurm-jobs">Slurm jobs</a></li>
<li class="toctree-l1"><a class="reference internal" href="readme_link.html#batching">Batching</a></li>
<li class="toctree-l1"><a class="reference internal" href="readme_link.html#using-the-gpu-on-slurm">Using the GPU on Slurm</a></li>
<li class="toctree-l1"><a class="reference internal" href="readme_link.html#transfering-data">Transfering data</a></li>
<li class="toctree-l1"><a class="reference internal" href="readme_link.html#testing-the-python-code">Testing the Python code</a></li>
<li class="toctree-l1"><a class="reference internal" href="readme_link.html#logging">Logging</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Cellprofiler tutorial</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#prerequisite-omero-slurm-and-biomero">0. Prerequisite: OMERO, Slurm and <code class="docutils literal notranslate"><span class="pre">biomero</span></code>.</a></li>
<li class="toctree-l2"><a class="reference internal" href="#grab-the-data-and-pipeline">1. Grab the data and pipeline</a></li>
<li class="toctree-l2"><a class="reference internal" href="#try-the-pipeline-locally">2. Try the pipeline locally</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#ui">UI</a></li>
<li class="toctree-l3"><a class="reference internal" href="#export-pipeline-file-only">Export pipeline file only</a></li>
<li class="toctree-l3"><a class="reference internal" href="#headless">headless</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#upload-the-data-to-omero">3. Upload the data to OMERO</a></li>
<li class="toctree-l2"><a class="reference internal" href="#package-the-cellprofiler-in-a-fair-package">4. Package the cellprofiler in a FAIR package</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#create-a-workflow-github-repository">0. Create a workflow Github repository</a></li>
<li class="toctree-l3"><a class="reference internal" href="#a-create-a-dockerfile-for-cellprofiler">a. Create a Dockerfile for cellprofiler</a></li>
<li class="toctree-l3"><a class="reference internal" href="#b-setup-the-metadata-in-descriptor-json">b. Setup the metadata in <code class="docutils literal notranslate"><span class="pre">descriptor.json</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#c-update-the-command-in-wrapper-py">c. Update the command in <code class="docutils literal notranslate"><span class="pre">wrapper.py</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="#d-run-locally">d. Run locally</a></li>
<li class="toctree-l3"><a class="reference internal" href="#e-publish-to-github-and-dockerhub">e. Publish to GitHub and DockerHub</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#optional-manually-publish-the-image-on-dockerhub">Optional: Manually publish the image on Dockerhub:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#add-this-workflow-to-the-omero-slurm-client">5. Add this workflow to the OMERO Slurm Client</a></li>
<li class="toctree-l2"><a class="reference internal" href="#add-a-omero-script-to-run-this-from-the-web-ui">6. Add a OMERO script to run this from the Web UI</a></li>
<li class="toctree-l2"><a class="reference internal" href="#extra-how-to-add-workflow-parameters-to-cellprofiler">Extra: How to add workflow parameters to cellprofiler?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#extra-2-we-should-add-a-license">Extra 2: We should add a LICENSE</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#cellexpansion-tutorial">CellExpansion tutorial</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#import-data-to-omero">1. Import data to OMERO</a></li>
<li class="toctree-l2"><a class="reference internal" href="#extract-masks-with-cellpose">2. Extract masks with Cellpose</a></li>
<li class="toctree-l2"><a class="reference internal" href="#cellexpansion">3. CellExpansion</a></li>
<li class="toctree-l2"><a class="reference internal" href="#calculate-overlap">Calculate overlap</a></li>
<li class="toctree-l2"><a class="reference internal" href="#workflow-management">Workflow management?</a></li>
<li class="toctree-l2"><a class="reference internal" href="#extra">Extra</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#out-of-memory">Out of memory</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#local-slurm-tutorial">Local Slurm tutorial</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id1">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#requirements">0. Requirements</a></li>
<li class="toctree-l2"><a class="reference internal" href="#setup-docker-containers-for-slurm">1. Setup Docker containers for Slurm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#tl-dr">TL;DR:</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#add-ssh-access">2. Add SSH access</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id2">TL;DR:</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#test-slurm">3. Test Slurm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id3">TL;DR:</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#omero-omero-slurm-client">4. OMERO &amp; OMERO Slurm Client</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id4">TL;DR:</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#workflows">5. Workflows!</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id5">TL;DR:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#batching">Batching</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#google-cloud-slurm-tutorial">Google Cloud Slurm tutorial</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id6">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id7">0. Requirements</a></li>
<li class="toctree-l2"><a class="reference internal" href="#setup-google-cloud-for-slurm">1. Setup Google Cloud for Slurm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id8">TL;DR:</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id9">2. Add SSH access</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id10">TL;DR:</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id11">3. Test Slurm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id12">TL;DR:</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#b-install-requirements-singularity-apptainer-and-7zip">3b. Install requirements: Singularity / Apptainer and 7zip</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id13">TL;DR:</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id14">4. OMERO &amp; OMERO Slurm Client</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id15">TL;DR:</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id16">5. Workflows!</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id17">TL;DR:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#microsoft-azure-slurm-tutorial">Microsoft Azure Slurm tutorial</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id18">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id19">0. Requirements</a></li>
<li class="toctree-l2"><a class="reference internal" href="#setup-microsoft-azure-for-slurm">1. Setup Microsoft Azure for Slurm</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id20">TL;DR:</a></li>
<li class="toctree-l3"><a class="reference internal" href="#suggested-alternative-use-a-basic-slurm-cluster">Suggested alternative: use a basic Slurm cluster</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#addendum-setting-up-slurm-accounting-db">1 - Addendum - Setting up Slurm Accounting DB</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#a-create-extra-subnet-on-your-virtual-network">A. Create extra subnet on your virtual network</a></li>
<li class="toctree-l3"><a class="reference internal" href="#b-azure-database">B. Azure database</a></li>
<li class="toctree-l3"><a class="reference internal" href="#c-slurm-accounting-settings">C. Slurm Accounting settings</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id21">2. Test Slurm</a></li>
<li class="toctree-l2"><a class="reference internal" href="#test-singularity-on-slurm">3. Test Singularity on Slurm</a></li>
<li class="toctree-l2"><a class="reference internal" href="#setting-up-bi-omero-in-azure-too-optional">5. Setting up (BI)OMERO in Azure too (Optional)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#showtime">6. Showtime!</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="#extra-thoughts">Extra thoughts</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Package</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="modules.html">biomero</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">BIOMERO</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content style-external-links">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Cellprofiler tutorial</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/tutorial_link.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <hr class="docutils" />
<div class="section" id="cellprofiler-tutorial">
<h1>Cellprofiler tutorial<a class="headerlink" href="#cellprofiler-tutorial" title="Permalink to this headline"></a></h1>
<p>Cellprofiler is already known for excellent interoperability with OMERO. You can directly load images into the cellprofiler pipelines.</p>
<p>Cellprofiler also has options to run in batch mode and headless, for analyzing big data on compute clusters, like we want as well.</p>
<p>However, for our purposes, this is insufficient, as we want to run it from OMERO, and on a compute cluster that only has SSH access.</p>
<p>In this tutorial I will show you how to add a cellprofiler pipeline as a workflow to OMERO and Slurm, with this client library.</p>
<div class="section" id="prerequisite-omero-slurm-and-biomero">
<h2>0. Prerequisite: OMERO, Slurm and <code class="docutils literal notranslate"><span class="pre">biomero</span></code>.<a class="headerlink" href="#prerequisite-omero-slurm-and-biomero" title="Permalink to this headline"></a></h2>
<p>We assume you have these 3 components setup and connected. If not, follow the main <a class="reference internal" href="#README.md"><span class="xref myst">README</span></a> first.</p>
</div>
<div class="section" id="grab-the-data-and-pipeline">
<h2>1. Grab the data and pipeline<a class="headerlink" href="#grab-the-data-and-pipeline" title="Permalink to this headline"></a></h2>
<p>We want to try something ready-made, and we like spots here at the AMC.</p>
<p>So let’s grab this spot-counting example from the cellprofiler website:
https://github.com/tischi/cellprofiler-practical-NeuBIAS-Lisbon-2017/blob/master/practical-handout.md</p>
</div>
<div class="section" id="try-the-pipeline-locally">
<h2>2. Try the pipeline locally<a class="headerlink" href="#try-the-pipeline-locally" title="Permalink to this headline"></a></h2>
<div class="section" id="ui">
<h3>UI<a class="headerlink" href="#ui" title="Permalink to this headline"></a></h3>
<p>It is always a good idea to test your algorithms locally before jumping to remote compute.
You can walk through the readme, or open the <a class="reference external" href="https://github.com/tischi/cellprofiler-practical-NeuBIAS-Lisbon-2017/blob/master/PLA-dot-counting-with-speckle-enhancement.cpproj">PLA-dot-counting-with-speckle-enhancement.cpproj</a>. It seems to be a bit older, so we have to fix the threshold (<code class="docutils literal notranslate"><span class="pre">(0.0</span></code> to <code class="docutils literal notranslate"><span class="pre">0.0</span></code>) and change the input to our local file location.</p>
</div>
<div class="section" id="export-pipeline-file-only">
<h3>Export pipeline file only<a class="headerlink" href="#export-pipeline-file-only" title="Permalink to this headline"></a></h3>
<p>Cellprofiler works with both <code class="docutils literal notranslate"><span class="pre">.cpproj</span></code> and <code class="docutils literal notranslate"><span class="pre">.cppipe</span></code>. The project version hardcodes the filepaths in there, which we don’t want. So go to <code class="docutils literal notranslate"><span class="pre">File</span></code> &gt; <code class="docutils literal notranslate"><span class="pre">Export</span></code> &gt; <code class="docutils literal notranslate"><span class="pre">Pipeline</span></code> and save this as a <code class="docutils literal notranslate"><span class="pre">.cppipe</span></code> file.</p>
<p>Another bonus is that <code class="docutils literal notranslate"><span class="pre">.cppipe</span></code> is human-readable and editable. Important later on.</p>
</div>
<div class="section" id="headless">
<h3>headless<a class="headerlink" href="#headless" title="Permalink to this headline"></a></h3>
<p>After it works, let’s try it headless too:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./cellprofiler.exe -c -p &#39;&lt;path-to&gt;\PLA-dot-counting-with-speckle-enhancement.cppipe&#39; -o &#39;&lt;path-to&gt;\cellprofiler_results&#39; -i &#39;&lt;path-to&gt;\PLA_data
</pre></div>
</div>
<p>Here we provide the input images (<code class="docutils literal notranslate"><span class="pre">-i</span></code>), the output folder (<code class="docutils literal notranslate"><span class="pre">-o</span></code>), the project (<code class="docutils literal notranslate"><span class="pre">-p</span></code>) and headless mode (<code class="docutils literal notranslate"><span class="pre">-c</span></code>).</p>
<p>See <a class="reference external" href="https://carpenter-singh-lab.broadinstitute.org/blog/getting-started-using-cellprofiler-command-line">this blog</a> for more info on the commandline parameters.</p>
</div>
</div>
<div class="section" id="upload-the-data-to-omero">
<h2>3. Upload the data to OMERO<a class="headerlink" href="#upload-the-data-to-omero" title="Permalink to this headline"></a></h2>
<p>Let’s make a screen out of these 8 wells, for fun.</p>
<ul class="simple">
<li><p>Open the importer.</p></li>
<li><p>Since there is no screen metadata in the files, first create a project and dataset in OMERO.</p></li>
<li><p>Import the PLA_data folder there.</p></li>
<li><p>Go to the Web UI.</p></li>
<li><p>Select the new dataset.</p></li>
<li><p>Activate script <code class="docutils literal notranslate"><span class="pre">Dataset</span> <span class="pre">to</span> <span class="pre">Plate</span></code> (under omero/util_scripts/).</p>
<ul>
<li><p>Fill in 8 wells per row (optional)</p></li>
<li><p>Screen: PLA_data</p></li>
</ul>
</li>
<li><p>Now we have a plate with 8 wells in a screen in OMERO.</p></li>
</ul>
</div>
<div class="section" id="package-the-cellprofiler-in-a-fair-package">
<h2>4. Package the cellprofiler in a FAIR package<a class="headerlink" href="#package-the-cellprofiler-in-a-fair-package" title="Permalink to this headline"></a></h2>
<p>To create a FAIR workflow, let’s follow the steps from Biaflows for creating a new workflow, as they explained it quite well already: https://neubias-wg5.github.io/creating_bia_workflow_and_adding_to_biaflows_instance.html</p>
<p>We just ignore some parts specific to the BIAFLOWS server, like adding as a trusted source. We will add the workflow to OMERO and Slurm instead, as a final step.</p>
<div class="section" id="create-a-workflow-github-repository">
<h3>0. Create a workflow Github repository<a class="headerlink" href="#create-a-workflow-github-repository" title="Permalink to this headline"></a></h3>
<p>To kickstart, we can reuse some of the workflow setup for CellProfiler from <a class="reference external" href="https://github.com/Neubias-WG5/W_NucleiSegmentation-CellProfiler">Neubias</a> github.</p>
<p>You can follow along, or just use my version at the end (https://github.com/TorecLuik/W_SpotCounting-CellProfiler)</p>
<ul class="simple">
<li><p>Login/create an account on Github</p></li>
<li><p>Go to link above.</p></li>
<li><p>Go to <code class="docutils literal notranslate"><span class="pre">Use</span> <span class="pre">this</span> <span class="pre">template</span></code></p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">Create</span> <span class="pre">a</span> <span class="pre">new</span> <span class="pre">repository</span></code></p>
<ul>
<li><p>Name it <code class="docutils literal notranslate"><span class="pre">W_SpotCounting-CellProfiler</span></code></p></li>
<li><p>Keep it Public</p></li>
</ul>
</li>
</ul>
</li>
<li><p>Clone your new repository locally</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">Code</span></code> &gt; <code class="docutils literal notranslate"><span class="pre">Clone</span></code> &gt; <code class="docutils literal notranslate"><span class="pre">HTTPS</span></code> &gt; Copy</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">git</span> <span class="pre">clone</span> <span class="pre">https://github.com/&lt;...&gt;/W_SpotCounting-CellProfiler.git</span></code></p></li>
</ul>
</li>
<li><p>Open the folder in your favorite <a class="reference external" href="https://code.visualstudio.com/">editor</a></p></li>
<li><p>Copy the project we want to this folder e.g. <code class="docutils literal notranslate"><span class="pre">PLA-dot-counting-with-speckle-enhancement.cpproj</span></code></p></li>
</ul>
</div>
<div class="section" id="a-create-a-dockerfile-for-cellprofiler">
<h3>a. Create a Dockerfile for cellprofiler<a class="headerlink" href="#a-create-a-dockerfile-for-cellprofiler" title="Permalink to this headline"></a></h3>
<p>The Dockerfile installs our whole environment.</p>
<p>We want:</p>
<ol class="arabic simple">
<li><p>Cellprofiler</p></li>
<li><p>Cytomine/Biaflows helper libraries (for Input/Output)</p></li>
<li><p>Our workflow files:</p></li>
</ol>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">wrapper.py</span></code> (the logic to run our workflow)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">descriptor.json</span></code> (the metadata of our workflow)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">*.cppipe</span></code> (our cellprofiler pipeline)</p></li>
</ul>
<p>Now it turns out that this <a class="reference external" href="https://github.com/Neubias-WG5/W_NucleiSegmentation-CellProfiler">Dockerfile</a> uses an old version of CellProfiler (with Python 2).
We want the newest one, so I rewrote the Dockerfile:</p>
<details>
  <summary>Our new/changed Dockerfile</summary>
<div class="highlight-Dockerfile notranslate"><div class="highlight"><pre><span></span><span class="k">FROM</span> <span class="s">cellprofiler/cellprofiler</span>
</pre></div>
</div>
<p>Instead of installing cellprofiler manually, it turns out they host containers images themselves, so let’s reuse <a class="reference external" href="https://hub.docker.com/r/cellprofiler/cellprofiler">those</a>.</p>
<div class="highlight-Dockerfile notranslate"><div class="highlight"><pre><span></span><span class="c"># Install Python3.7</span>
<span class="k">RUN</span> apt-get update <span class="o">&amp;&amp;</span> apt-get install -y python3.7 python3.7-dev python3.7-venv
<span class="k">RUN</span> python3.7 -m pip install --upgrade pip <span class="o">&amp;&amp;</span> python3.7 -m pip install Cython
</pre></div>
</div>
<p>This cellprofiler image is quite modern, but we need an older Python to work with the Cytomine/Biaflows libraries. So we Install Python3.7 (and Cython package).</p>
<div class="highlight-Dockerfile notranslate"><div class="highlight"><pre><span></span><span class="c"># ------------------------------------------------------------------------------</span>
<span class="c"># Install Cytomine python client</span>
<span class="k">RUN</span> git clone https://github.com/cytomine-uliege/Cytomine-python-client.git <span class="o">&amp;&amp;</span> <span class="se">\</span>
    <span class="nb">cd</span> Cytomine-python-client <span class="o">&amp;&amp;</span> git checkout tags/v2.7.3 <span class="o">&amp;&amp;</span> <span class="se">\ </span>
    python3.7 -m pip install . <span class="o">&amp;&amp;</span> <span class="se">\</span>
    <span class="nb">cd</span> .. <span class="o">&amp;&amp;</span> <span class="se">\</span>
    rm -r Cytomine-python-client

<span class="c"># ------------------------------------------------------------------------------</span>
<span class="c"># Install BIAFLOWS utilities (annotation exporter, compute metrics, helpers,...)</span>
<span class="k">RUN</span> apt-get update <span class="o">&amp;&amp;</span> apt-get install libgeos-dev -y <span class="o">&amp;&amp;</span> apt-get clean
<span class="k">RUN</span> git clone https://github.com/Neubias-WG5/biaflows-utilities.git <span class="o">&amp;&amp;</span> <span class="se">\</span>
    <span class="nb">cd</span> biaflows-utilities/ <span class="o">&amp;&amp;</span> git checkout tags/v0.9.1 <span class="o">&amp;&amp;</span> python3.7 -m pip install .

<span class="c"># install utilities binaries</span>
<span class="k">RUN</span> chmod +x biaflows-utilities/bin/*
<span class="k">RUN</span> cp biaflows-utilities/bin/* /usr/bin/ <span class="o">&amp;&amp;</span> <span class="se">\</span>
    rm -r biaflows-utilities
</pre></div>
</div>
<p>These 2 parts install specific versions of the biaflows library and Cytomine library with Python 3.7.</p>
<div class="highlight-Dockerfile notranslate"><div class="highlight"><pre><span></span><span class="c"># ------------------------------------------------------------------------------</span>
<span class="c"># Add repository files: wrapper, command and descriptor</span>
<span class="k">RUN</span> mkdir /app
<span class="k">ADD</span> wrapper.py /app/wrapper.py
<span class="k">ADD</span> PLA-dot-counting-with-speckle-enhancement.cppipe /app/PLA-dot-counting-with-speckle-enhancement.cppipe
<span class="k">ADD</span> descriptor.json /app/descriptor.json

<span class="k">ENTRYPOINT</span> <span class="p">[</span><span class="s2">&quot;python3.7&quot;</span><span class="p">,</span><span class="s2">&quot;/app/wrapper.py&quot;</span><span class="p">]</span>
</pre></div>
</div>
<p>Finally we add our own workflow to <code class="docutils literal notranslate"><span class="pre">/app</span></code> folder:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">wrapper.py</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">.cppipe</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">descriptor.json</span></code></p></li>
</ul>
<p>And we tell the image to call <code class="docutils literal notranslate"><span class="pre">wrapper.py</span></code> with python3.7 when we start it up using an <code class="docutils literal notranslate"><span class="pre">ENTRYPOINT</span></code>. This also forwards commandline parameters that you provide to the <code class="docutils literal notranslate"><span class="pre">wrapper.py</span></code> script, e.g. workflow parameters.</p>
</details>
</div>
<div class="section" id="b-setup-the-metadata-in-descriptor-json">
<h3>b. Setup the metadata in <code class="docutils literal notranslate"><span class="pre">descriptor.json</span></code><a class="headerlink" href="#b-setup-the-metadata-in-descriptor-json" title="Permalink to this headline"></a></h3>
<p>We actually don’t have any input parameters (except the default input/output) at this moment. Look at <a class="reference internal" href="#extra-how-to-add-workflow-parameters-to-cellprofiler"><span class="xref myst">this</span></a> extra chapter for more info on how to approach that.</p>
<p>So we can just use the basic <code class="docutils literal notranslate"><span class="pre">descriptor.json</span></code> that was given and remove the last 2 non-cytomine parameters.
Mainly, update the name, description and where we will publish the container (your new dockerhub account).</p>
<details>
  <summary>Example full json</summary>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
  <span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;SpotCounting-CellProfiler&quot;</span><span class="p">,</span>
  <span class="nt">&quot;description&quot;</span><span class="p">:</span> <span class="s2">&quot;Workflow for spot counting in CellProfiler&quot;</span><span class="p">,</span>
  <span class="nt">&quot;container-image&quot;</span><span class="p">:</span> <span class="p">{</span>
    <span class="nt">&quot;image&quot;</span><span class="p">:</span> <span class="s2">&quot;torecluik/w_spotcounting-cellprofiler&quot;</span><span class="p">,</span>
    <span class="nt">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;singularity&quot;</span>
  <span class="p">},</span>
  <span class="nt">&quot;command-line&quot;</span><span class="p">:</span> <span class="s2">&quot;python wrapper.py CYTOMINE_HOST CYTOMINE_PUBLIC_KEY CYTOMINE_PRIVATE_KEY CYTOMINE_ID_PROJECT CYTOMINE_ID_SOFTWARE&quot;</span><span class="p">,</span>
  <span class="nt">&quot;inputs&quot;</span><span class="p">:</span> <span class="p">[</span>
    <span class="p">{</span>
      <span class="nt">&quot;id&quot;</span><span class="p">:</span> <span class="s2">&quot;cytomine_host&quot;</span><span class="p">,</span>
      <span class="nt">&quot;value-key&quot;</span><span class="p">:</span> <span class="s2">&quot;@ID&quot;</span><span class="p">,</span>
      <span class="nt">&quot;command-line-flag&quot;</span><span class="p">:</span> <span class="s2">&quot;--@id&quot;</span><span class="p">,</span>
      <span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;BIAFLOWS host&quot;</span><span class="p">,</span>
      <span class="nt">&quot;set-by-server&quot;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
      <span class="nt">&quot;optional&quot;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
      <span class="nt">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;String&quot;</span>
    <span class="p">},</span>
    <span class="p">{</span>
      <span class="nt">&quot;id&quot;</span><span class="p">:</span> <span class="s2">&quot;cytomine_public_key&quot;</span><span class="p">,</span>
      <span class="nt">&quot;value-key&quot;</span><span class="p">:</span> <span class="s2">&quot;@ID&quot;</span><span class="p">,</span>
      <span class="nt">&quot;command-line-flag&quot;</span><span class="p">:</span> <span class="s2">&quot;--@id&quot;</span><span class="p">,</span>
      <span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;BIAFLOWS public key&quot;</span><span class="p">,</span>
      <span class="nt">&quot;set-by-server&quot;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
      <span class="nt">&quot;optional&quot;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
      <span class="nt">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;String&quot;</span>
    <span class="p">},</span>
    <span class="p">{</span>
      <span class="nt">&quot;id&quot;</span><span class="p">:</span> <span class="s2">&quot;cytomine_private_key&quot;</span><span class="p">,</span>
      <span class="nt">&quot;value-key&quot;</span><span class="p">:</span> <span class="s2">&quot;@ID&quot;</span><span class="p">,</span>
      <span class="nt">&quot;command-line-flag&quot;</span><span class="p">:</span> <span class="s2">&quot;--@id&quot;</span><span class="p">,</span>
      <span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;BIAFLOWS private key&quot;</span><span class="p">,</span>
      <span class="nt">&quot;set-by-server&quot;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
      <span class="nt">&quot;optional&quot;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
      <span class="nt">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;String&quot;</span>
    <span class="p">},</span>
    <span class="p">{</span>
      <span class="nt">&quot;id&quot;</span><span class="p">:</span> <span class="s2">&quot;cytomine_id_project&quot;</span><span class="p">,</span>
      <span class="nt">&quot;value-key&quot;</span><span class="p">:</span> <span class="s2">&quot;@ID&quot;</span><span class="p">,</span>
      <span class="nt">&quot;command-line-flag&quot;</span><span class="p">:</span> <span class="s2">&quot;--@id&quot;</span><span class="p">,</span>
      <span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;BIAFLOWS project ID&quot;</span><span class="p">,</span>
      <span class="nt">&quot;set-by-server&quot;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
      <span class="nt">&quot;optional&quot;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
      <span class="nt">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;Number&quot;</span>
    <span class="p">},</span>
    <span class="p">{</span>
      <span class="nt">&quot;id&quot;</span><span class="p">:</span> <span class="s2">&quot;cytomine_id_software&quot;</span><span class="p">,</span>
      <span class="nt">&quot;value-key&quot;</span><span class="p">:</span> <span class="s2">&quot;@ID&quot;</span><span class="p">,</span>
      <span class="nt">&quot;command-line-flag&quot;</span><span class="p">:</span> <span class="s2">&quot;--@id&quot;</span><span class="p">,</span>
      <span class="nt">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;BIAFLOWS software ID&quot;</span><span class="p">,</span>
      <span class="nt">&quot;set-by-server&quot;</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
      <span class="nt">&quot;optional&quot;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
      <span class="nt">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;Number&quot;</span>
    <span class="p">}</span>
  <span class="p">],</span>

  <span class="nt">&quot;schema-version&quot;</span><span class="p">:</span> <span class="s2">&quot;cytomine-0.1&quot;</span>
<span class="p">}</span>
</pre></div>
</div>
</details>
</div>
<div class="section" id="c-update-the-command-in-wrapper-py">
<h3>c. Update the command in <code class="docutils literal notranslate"><span class="pre">wrapper.py</span></code><a class="headerlink" href="#c-update-the-command-in-wrapper-py" title="Permalink to this headline"></a></h3>
<p>So the wrapper gets called when the container starts.
This is where we ‘wrap’ our pipeline by handling input/output and parameters.
We also have to make sure that we call the pipeline correctly here.</p>
<details>
  <summary>Our changes to the wrapper</summary>
<p>This first part we keep the same: the <code class="docutils literal notranslate"><span class="pre">BiaflowsJob</span></code> will parse the commandline parameters for us and provide those as <code class="docutils literal notranslate"><span class="pre">bj.parameter.&lt;param_name&gt;</span></code> if we did want them. But we don’t use any right now.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">argv</span><span class="p">):</span>
    <span class="n">base_path</span> <span class="o">=</span> <span class="s2">&quot;</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;HOME&quot;</span><span class="p">))</span> <span class="c1"># Mandatory for Singularity</span>
    <span class="n">problem_cls</span> <span class="o">=</span> <span class="n">CLASS_OBJSEG</span>

    <span class="k">with</span> <span class="n">BiaflowsJob</span><span class="o">.</span><span class="n">from_cli</span><span class="p">(</span><span class="n">argv</span><span class="p">)</span> <span class="k">as</span> <span class="n">bj</span><span class="p">:</span>
        <span class="n">bj</span><span class="o">.</span><span class="n">job</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">status</span><span class="o">=</span><span class="n">Job</span><span class="o">.</span><span class="n">RUNNING</span><span class="p">,</span> <span class="n">progress</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">statusComment</span><span class="o">=</span><span class="s2">&quot;Initialisation...&quot;</span><span class="p">)</span>
        <span class="c1"># 1. Prepare data for workflow</span>
        <span class="n">in_imgs</span><span class="p">,</span> <span class="n">gt_imgs</span><span class="p">,</span> <span class="n">in_path</span><span class="p">,</span> <span class="n">gt_path</span><span class="p">,</span> <span class="n">out_path</span><span class="p">,</span> <span class="n">tmp_path</span> <span class="o">=</span> <span class="n">prepare_data</span><span class="p">(</span><span class="n">problem_cls</span><span class="p">,</span> <span class="n">bj</span><span class="p">,</span> <span class="n">is_2d</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">bj</span><span class="o">.</span><span class="n">flags</span><span class="p">)</span>
</pre></div>
</div>
<p>The second part (where we call our pipeline) we can simplify a bit, as we don’t need to parse parameters for cellprofiler. See <a class="reference internal" href="#extra-how-to-add-workflow-parameters-to-cellprofiler"><span class="xref myst">later</span></a> for how to start handling that.</p>
<p>We specifically name the cppipe that we added to <code class="docutils literal notranslate"><span class="pre">/app</span></code>, and we use <code class="docutils literal notranslate"><span class="pre">subprocess.run(...)</span></code> to execute our cellprofiler headless on the commandline: <code class="docutils literal notranslate"><span class="pre">cellprofiler</span> <span class="pre">-c</span> <span class="pre">-r</span> <span class="pre">-p</span> <span class="pre">...</span> <span class="pre">-i</span> <span class="pre">...</span> <span class="pre">-o</span> <span class="pre">...</span> <span class="pre">-t</span></code>.</p>
<p>In theory we could also use the cellprofiler python package here, for more control. But in general, we can run any commandline program with <code class="docutils literal notranslate"><span class="pre">subprocess.run</span></code>, so this wrapper will look similar for most workflows.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>        <span class="n">pipeline</span> <span class="o">=</span> <span class="s2">&quot;/app/PLA-dot-counting-with-speckle-enhancement.cppipe&quot;</span>

        <span class="c1"># 2. Run CellProfiler pipeline</span>
        <span class="n">bj</span><span class="o">.</span><span class="n">job</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">progress</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">statusComment</span><span class="o">=</span><span class="s2">&quot;Launching workflow...&quot;</span><span class="p">)</span>
        
        <span class="c1">## If we want to allow parameters, we have to parse them into the pipeline here</span>
        <span class="c1"># mod_pipeline = parse_cellprofiler_parameters(bj, pipeline, tmp_path)</span>
        <span class="n">mod_pipeline</span> <span class="o">=</span> <span class="n">pipeline</span>

        <span class="n">shArgs</span> <span class="o">=</span> <span class="p">[</span>
            <span class="s2">&quot;cellprofiler&quot;</span><span class="p">,</span> <span class="s2">&quot;-c&quot;</span><span class="p">,</span> <span class="s2">&quot;-r&quot;</span><span class="p">,</span> <span class="s2">&quot;-p&quot;</span><span class="p">,</span> <span class="n">mod_pipeline</span><span class="p">,</span>
            <span class="s2">&quot;-i&quot;</span><span class="p">,</span> <span class="n">in_path</span><span class="p">,</span> <span class="s2">&quot;-o&quot;</span><span class="p">,</span> <span class="n">out_path</span><span class="p">,</span> <span class="s2">&quot;-t&quot;</span><span class="p">,</span> <span class="n">tmp_path</span><span class="p">,</span>
        <span class="p">]</span>
        <span class="n">status</span> <span class="o">=</span> <span class="n">run</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">shArgs</span><span class="p">),</span> <span class="n">shell</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>Finally, we don’t change much to the rest of the script and just handle the return code. 0 means success, so then we just log to the logfile.</p>
<p>There is some built-in logic for <code class="docutils literal notranslate"><span class="pre">Biaflows</span></code>, like uploading results and metrics.
We keep it in for the logs, but they are essentially a <a class="reference external" href="https://en.wikipedia.org/wiki/NOP_(code)"><code class="docutils literal notranslate"><span class="pre">no-op</span></code></a> because we will provide the command-line parameters <code class="docutils literal notranslate"><span class="pre">--local</span></code> and <code class="docutils literal notranslate"><span class="pre">-nmc</span></code> (<code class="docutils literal notranslate"><span class="pre">n</span></code>o <code class="docutils literal notranslate"><span class="pre">m</span></code>etric <code class="docutils literal notranslate"><span class="pre">c</span></code>omputation).</p>
</details>
<p>Full changes can be found <a class="reference external" href="https://github.com/TorecLuik/W_SpotCounting-CellProfiler/blob/master/wrapper.py">here</a></p>
</div>
<div class="section" id="d-run-locally">
<h3>d. Run locally<a class="headerlink" href="#d-run-locally" title="Permalink to this headline"></a></h3>
<p>Now that we have a docker, we can run this locally or anywhere that we have docker installed, without the need for having the right version of cellprofiler, etc.
Let’s try it out:</p>
<ol class="arabic simple" start="0">
<li><p>Setup your data folder like this:</p></li>
</ol>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">PLA</span></code> as main folder</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">PLA_data</span></code> with the 8 images, as subfolder</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">out</span></code> as empty subfolder</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gt</span></code> as empty subfolder</p></li>
</ul>
</li>
</ul>
<ol class="arabic simple">
<li><p>Build a container: <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">build</span> <span class="pre">-t</span> <span class="pre">spotcounting-cp</span> <span class="pre">.</span></code> (Note the <code class="docutils literal notranslate"><span class="pre">.</span></code> is important, it means <code class="docutils literal notranslate"><span class="pre">this</span> <span class="pre">folder</span></code>)</p></li>
<li><p>Run the container on the <code class="docutils literal notranslate"><span class="pre">PLA</span></code> folder like this: <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">run</span> <span class="pre">--rm</span> <span class="pre">-v</span> <span class="pre">&lt;my-drive&gt;\PLA\:/data-it</span> <span class="pre">spotcounting-cp</span> <span class="pre">--local</span> <span class="pre">--infolder</span> <span class="pre">/data-it/PLA_data</span> <span class="pre">--outfolder</span> <span class="pre">/data-it/out</span> <span class="pre">--gtfolder</span> <span class="pre">/data-it/gt</span>&#160; <span class="pre">-nmc</span> </code></p></li>
</ol>
<p>This should work the same as <a class="reference internal" href="#2-try-the-pipeline-locally"><span class="xref myst">before</span></a>, with a bit of extra logging thrown in.
Except now, we didn’t need to have cellprofiler installed! Anyone with <code class="docutils literal notranslate"><span class="pre">Docker</span></code> (or <code class="docutils literal notranslate"><span class="pre">Podman</span></code> or <code class="docutils literal notranslate"><span class="pre">Singularity</span></code>) can run this workflow now.</p>
</div>
<div class="section" id="e-publish-to-github-and-dockerhub">
<h3>e. Publish to GitHub and DockerHub<a class="headerlink" href="#e-publish-to-github-and-dockerhub" title="Permalink to this headline"></a></h3>
<p>So how do other people get to use our workflow?</p>
<ol class="arabic simple">
<li><p>We publish the source online on Github:</p></li>
</ol>
<ul class="simple">
<li><p>Commit to git: <code class="docutils literal notranslate"><span class="pre">git</span> <span class="pre">commit</span> <span class="pre">-m</span> <span class="pre">'Update</span> <span class="pre">with</span> <span class="pre">spotcounting</span> <span class="pre">pipeline'</span> <span class="pre">-a</span></code></p></li>
<li><p>Push to github: <code class="docutils literal notranslate"><span class="pre">git</span> <span class="pre">push</span></code></p></li>
<li><p>Setup automated release to Dockerhub:</p>
<ul>
<li><p>First, create a free account on Dockerhub if you don’t have one</p></li>
<li><p>On Dockerhub, login and create a new <code class="docutils literal notranslate"><span class="pre">Access</span> <span class="pre">Token</span></code> via <code class="docutils literal notranslate"><span class="pre">Account</span> <span class="pre">Settings</span></code> / <code class="docutils literal notranslate"><span class="pre">Security</span></code>. Name it <code class="docutils literal notranslate"><span class="pre">Github</span></code> or something. Copy this token (to a file).</p></li>
<li><p>Back on your Github repository, add 2 secrets by going to <code class="docutils literal notranslate"><span class="pre">Settings</span></code> / <code class="docutils literal notranslate"><span class="pre">Secrets</span> <span class="pre">and</span> <span class="pre">variables</span></code> / <code class="docutils literal notranslate"><span class="pre">Actions</span></code> / <code class="docutils literal notranslate"><span class="pre">New</span> <span class="pre">repository</span> <span class="pre">secret</span></code></p>
<ul>
<li><p>First, add Name: <code class="docutils literal notranslate"><span class="pre">DOCKERHUB_USERNAME</span></code> and Secret: <code class="docutils literal notranslate"><span class="pre">&lt;your-dockerhub-username&gt;</span></code></p></li>
<li><p>Also, add Name: <code class="docutils literal notranslate"><span class="pre">DOCKERHUB_TOKEN</span></code> and Secret: <code class="docutils literal notranslate"><span class="pre">&lt;token-that-you-copied&gt;</span></code></p></li>
</ul>
</li>
</ul>
</li>
<li><p>Now, tag and release this as a new version on Github (and automatically Dockerhub):</p>
<ul>
<li><p>Pretty easy to do from Github page: <code class="docutils literal notranslate"><span class="pre">Releases</span></code> &gt; <code class="docutils literal notranslate"><span class="pre">new</span> <span class="pre">release</span></code>.</p></li>
<li><p>Add a tag like <code class="docutils literal notranslate"><span class="pre">v1.0.0</span></code>.</p></li>
<li><p>Now, the Github Action <code class="docutils literal notranslate"><span class="pre">Docker</span> <span class="pre">Image</span> <span class="pre">CI</span></code> will build the container for you and publish it on Dockerhub via the credentials you provided. This will take a few minutes, you can follow along at the <code class="docutils literal notranslate"><span class="pre">Actions</span></code> tab.</p></li>
<li><p>Now you can verify that it is available online:
https://hub.docker.com/u/your-dockerhub-user</p></li>
</ul>
</li>
<li><p>Great! now everybody (with internet access) can pull your workflow image and run it <a class="reference internal" href="#d-run-locally"><span class="xref myst">locally</span></a>: <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">run</span> <span class="pre">--rm</span> <span class="pre">-v</span> <span class="pre">&lt;my-drive&gt;\PLA\:/data-it</span> <span class="pre">&lt;your-dockerhub-user&gt;/w_spotcounting-cellprofiler:v1.0.0</span> <span class="pre">--local</span> <span class="pre">--infolder</span> <span class="pre">/data-it/PLA_data</span> <span class="pre">--outfolder</span> <span class="pre">/data-it/out</span> <span class="pre">--gtfolder</span> <span class="pre">/data-it/gt</span> <span class="pre">-nmc</span></code></p></li>
</ul>
<p>And this is what we will make OMERO do on the Slurm cluster next.</p>
<div class="section" id="optional-manually-publish-the-image-on-dockerhub">
<h4>Optional: Manually publish the image on Dockerhub:<a class="headerlink" href="#optional-manually-publish-the-image-on-dockerhub" title="Permalink to this headline"></a></h4>
<ul class="simple">
<li><p>First, create an account on Dockerhub if you don’t have one</p></li>
<li><p>Login locally on the commandline to this account too: <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">login</span></code></p></li>
<li><p>(Optional) Build your latest docker image if you didn’t do that yet (<code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">build</span> <span class="pre">-t</span> <span class="pre">spotcounting-cp</span> <span class="pre">.</span></code>).</p></li>
<li><p>Tag your local Docker image with a new tag to match this Dockerhub account and release: <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">tag</span> <span class="pre">spotcounting-cp:latest</span> <span class="pre">&lt;your-dockerhub-user&gt;/w_spotcounting-cellprofiler:v1.0.0</span></code></p></li>
<li><p>Push your tagged image to Dockerhub: <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">push</span> <span class="pre">&lt;your-dockerhub-user&gt;/w_spotcounting-cellprofiler:v1.0.0</span></code></p></li>
<li><p>Now you can verify that it is available online:
https://hub.docker.com/u/your-dockerhub-user</p></li>
</ul>
<p>E.g. mine can be found &#64; https://hub.docker.com/r/torecluik/w_spotcounting-cellprofiler/tags</p>
<ul class="simple">
<li><p>Great! now everybody (with internet access) can pull your workflow image and run it <a class="reference internal" href="#d-run-locally"><span class="xref myst">locally</span></a>: <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">run</span> <span class="pre">--rm</span> <span class="pre">-v</span> <span class="pre">&lt;my-drive&gt;\PLA\:/data-it</span> <span class="pre">&lt;your-dockerhub-user&gt;/w_spotcounting-cellprofiler:v1.0.0</span> <span class="pre">--local</span> <span class="pre">--infolder</span> <span class="pre">/data-it/PLA_data</span> <span class="pre">--outfolder</span> <span class="pre">/data-it/out</span> <span class="pre">--gtfolder</span> <span class="pre">/data-it/gt</span> <span class="pre">-nmc</span></code></p></li>
</ul>
<p>And this is what we will make OMERO do on the Slurm cluster next.</p>
</div>
</div>
</div>
<div class="section" id="add-this-workflow-to-the-omero-slurm-client">
<h2>5. Add this workflow to the OMERO Slurm Client<a class="headerlink" href="#add-this-workflow-to-the-omero-slurm-client" title="Permalink to this headline"></a></h2>
<ol class="arabic simple">
<li><p>Let’s adjust the <code class="docutils literal notranslate"><span class="pre">slurm-config.ini</span></code> on our OMERO processor server.</p></li>
</ol>
<p>In the <code class="docutils literal notranslate"><span class="pre">[MODEL]</span></code> section we add our new workflow:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="c1"># -------------------------------------</span>
<span class="c1"># CELLPROFILER SPOT COUNTING</span>
<span class="c1"># -------------------------------------</span>
<span class="c1"># The path to store the container on the slurm_images_path</span>
<span class="na">cellprofiler_spot</span><span class="o">=</span><span class="s">cellprofiler_spot</span>
<span class="c1"># The (e.g. github) repository with the descriptor.json file</span>
<span class="na">cellprofiler_spot_repo</span><span class="o">=</span><span class="s">https://github.com/TorecLuik/W_SpotCounting-CellProfiler/tree/v1.0.0</span>
<span class="c1"># The jobscript in the &#39;slurm_script_repo&#39;</span>
<span class="na">cellprofiler_spot_job</span><span class="o">=</span><span class="s">jobs/cellprofiler_spot.sh</span>
</pre></div>
</div>
<p>Note that we link to the <code class="docutils literal notranslate"><span class="pre">v1.0.0</span></code> specifically.</p>
<p>When using a new version, like <code class="docutils literal notranslate"><span class="pre">v1.0.1</span></code>, update this config again.
For example, I had a bugfix, so I released <a class="reference external" href="https://hub.docker.com/r/torecluik/w_spotcounting-cellprofiler/tags">my workflow</a> to <code class="docutils literal notranslate"><span class="pre">v1.0.1</span></code>, using the release + push + update steps.</p>
<p>For me, updating is done by rebuilding my docker container for the processor worker:
<code class="docutils literal notranslate"><span class="pre">docker-compose</span> <span class="pre">up</span> <span class="pre">-d</span> <span class="pre">--build</span> <span class="pre">omeroworker-5</span></code></p>
<ol class="arabic simple" start="2">
<li><p>and recreate the Slurm environment:</p></li>
</ol>
<ul class="simple">
<li><p>Run <code class="docutils literal notranslate"><span class="pre">SlurmClient.from_config(init_slurm=true)</span></code> on the OMERO processor server.</p></li>
</ul>
<details>
  <summary>E.g. using this omero script</summary>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/env python</span>
<span class="c1"># -*- coding: utf-8 -*-</span>
<span class="c1">#</span>
<span class="c1"># Original work Copyright (C) 2014 University of Dundee</span>
<span class="c1">#                                   &amp; Open Microscopy Environment.</span>
<span class="c1">#                    All Rights Reserved.</span>
<span class="c1"># Modified work Copyright 2022 Torec Luik, Amsterdam UMC</span>
<span class="c1"># Use is subject to license terms supplied in LICENSE.txt</span>
<span class="c1">#</span>
<span class="c1"># Example OMERO.script to instantiate a &#39;empty&#39; Slurm connection.</span>

<span class="kn">import</span> <span class="nn">omero</span>
<span class="kn">import</span> <span class="nn">omero.gateway</span>
<span class="kn">from</span> <span class="nn">omero</span> <span class="kn">import</span> <span class="n">scripts</span>
<span class="kn">from</span> <span class="nn">omero.rtypes</span> <span class="kn">import</span> <span class="n">rstring</span><span class="p">,</span> <span class="n">unwrap</span>
<span class="kn">from</span> <span class="nn">biomero</span> <span class="kn">import</span> <span class="n">SlurmClient</span>
<span class="kn">import</span> <span class="nn">logging</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">runScript</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The main entry point of the script</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">client</span> <span class="o">=</span> <span class="n">scripts</span><span class="o">.</span><span class="n">client</span><span class="p">(</span>
        <span class="s1">&#39;Slurm Init&#39;</span><span class="p">,</span>
        <span class="sd">&#39;&#39;&#39;Will initiate the Slurm environment for workflow execution.</span>

<span class="sd">        You can provide a config file location, </span>
<span class="sd">        and/or it will look for default locations:</span>
<span class="sd">        /etc/slurm-config.ini</span>
<span class="sd">        ~/slurm-config.ini</span>
<span class="sd">        &#39;&#39;&#39;</span><span class="p">,</span>
        <span class="n">scripts</span><span class="o">.</span><span class="n">Bool</span><span class="p">(</span><span class="s2">&quot;Init Slurm&quot;</span><span class="p">,</span> <span class="n">grouping</span><span class="o">=</span><span class="s2">&quot;01&quot;</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
        <span class="n">scripts</span><span class="o">.</span><span class="n">String</span><span class="p">(</span><span class="s2">&quot;Config file&quot;</span><span class="p">,</span> <span class="n">optional</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">grouping</span><span class="o">=</span><span class="s2">&quot;01.1&quot;</span><span class="p">,</span>
                       <span class="n">description</span><span class="o">=</span><span class="s2">&quot;The path to your configuration file. Optional.&quot;</span><span class="p">),</span>
        <span class="n">namespaces</span><span class="o">=</span><span class="p">[</span><span class="n">omero</span><span class="o">.</span><span class="n">constants</span><span class="o">.</span><span class="n">namespaces</span><span class="o">.</span><span class="n">NSDYNAMIC</span><span class="p">],</span>
    <span class="p">)</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="n">message</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
        <span class="n">init_slurm</span> <span class="o">=</span> <span class="n">unwrap</span><span class="p">(</span><span class="n">client</span><span class="o">.</span><span class="n">getInput</span><span class="p">(</span><span class="s2">&quot;Init Slurm&quot;</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">init_slurm</span><span class="p">:</span>
            <span class="n">configfile</span> <span class="o">=</span> <span class="n">unwrap</span><span class="p">(</span><span class="n">client</span><span class="o">.</span><span class="n">getInput</span><span class="p">(</span><span class="s2">&quot;Config file&quot;</span><span class="p">))</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">configfile</span><span class="p">:</span>
                <span class="n">configfile</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
            <span class="k">with</span> <span class="n">SlurmClient</span><span class="o">.</span><span class="n">from_config</span><span class="p">(</span><span class="n">configfile</span><span class="o">=</span><span class="n">configfile</span><span class="p">,</span>
                                         <span class="n">init_slurm</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">slurmClient</span><span class="p">:</span>
                <span class="n">slurmClient</span><span class="o">.</span><span class="n">validate</span><span class="p">(</span><span class="n">validate_slurm_setup</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="n">message</span> <span class="o">=</span> <span class="s2">&quot;Slurm is setup:&quot;</span>
                <span class="n">models</span><span class="p">,</span> <span class="n">data</span> <span class="o">=</span> <span class="n">slurmClient</span><span class="o">.</span><span class="n">get_all_image_versions_and_data_files</span><span class="p">()</span>
                <span class="n">message</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;Models: </span><span class="si">{models}</span><span class="se">\n</span><span class="s2">Data:</span><span class="si">{data}</span><span class="s2">&quot;</span>

        <span class="n">client</span><span class="o">.</span><span class="n">setOutput</span><span class="p">(</span><span class="s2">&quot;Message&quot;</span><span class="p">,</span> <span class="n">rstring</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">message</span><span class="p">)))</span>

    <span class="k">finally</span><span class="p">:</span>
        <span class="n">client</span><span class="o">.</span><span class="n">closeSession</span><span class="p">()</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">runScript</span><span class="p">()</span>

</pre></div>
</div>
</details>
<p>Now your Slurm cluster has</p>
<ul class="simple">
<li><p>your image ‘v1.0.0’.</p></li>
<li><p>And also a job-script for Slurm, automatically generated (unless you changed that behaviour in the <code class="docutils literal notranslate"><span class="pre">slurm-config</span></code>).</p></li>
</ul>
</div>
<div class="section" id="add-a-omero-script-to-run-this-from-the-web-ui">
<h2>6. Add a OMERO script to run this from the Web UI<a class="headerlink" href="#add-a-omero-script-to-run-this-from-the-web-ui" title="Permalink to this headline"></a></h2>
<ol class="arabic simple">
<li><p>select a screen / dataset</p></li>
<li><p>select workflow</p></li>
<li><p>run workflow!</p></li>
<li><p>check progress</p></li>
<li><p>Import resulting data</p></li>
</ol>
<p>I have created several OMERO scripts using this library, and the <a class="reference internal" href="#"><span class="xref myst"><code class="docutils literal notranslate"><span class="pre">run_workflow</span></code></span></a> can do this for us.
It will attach the results as a zipfile attachment to the screen.
Perhaps we can integrate with OMERO.Tables in the future.</p>
</div>
<div class="section" id="extra-how-to-add-workflow-parameters-to-cellprofiler">
<h2>Extra: How to add workflow parameters to cellprofiler?<a class="headerlink" href="#extra-how-to-add-workflow-parameters-to-cellprofiler" title="Permalink to this headline"></a></h2>
<p>So normally, adding workflow parameters to your commandline in <code class="docutils literal notranslate"><span class="pre">wrapper.py</span></code> is easy, like <a class="reference external" href="https://github.com/Neubias-WG5/W_NucleiSegmentation-Cellpose/blob/master/wrapper.py#L62C8-L65C37">this</a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>        <span class="c1"># Add here the code for running the analysis script</span>
        <span class="c1">#&quot;--chan&quot;, &quot;{:d}&quot;.format(nuc_channel)</span>
        <span class="n">cmd</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;python&quot;</span><span class="p">,</span> <span class="s2">&quot;-m&quot;</span><span class="p">,</span> <span class="s2">&quot;cellpose&quot;</span><span class="p">,</span> <span class="s2">&quot;--dir&quot;</span><span class="p">,</span> <span class="n">tmp_path</span><span class="p">,</span> <span class="s2">&quot;--pretrained_model&quot;</span><span class="p">,</span> <span class="s2">&quot;nuclei&quot;</span><span class="p">,</span> <span class="s2">&quot;--save_tif&quot;</span><span class="p">,</span> <span class="s2">&quot;--no_npy&quot;</span><span class="p">,</span> <span class="s2">&quot;--chan&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{:d}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">nuc_channel</span><span class="p">),</span> <span class="s2">&quot;--diameter&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{:f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">bj</span><span class="o">.</span><span class="n">parameters</span><span class="o">.</span><span class="n">diameter</span><span class="p">),</span> <span class="s2">&quot;--cellprob_threshold&quot;</span><span class="p">,</span> <span class="s2">&quot;</span><span class="si">{:f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">bj</span><span class="o">.</span><span class="n">parameters</span><span class="o">.</span><span class="n">prob_threshold</span><span class="p">)]</span>
        <span class="n">status</span> <span class="o">=</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">cmd</span><span class="p">)</span>
</pre></div>
</div>
<p>Here we add <code class="docutils literal notranslate"><span class="pre">bj.parameters.diameter</span></code> (described <a class="reference external" href="https://github.com/Neubias-WG5/W_NucleiSegmentation-Cellpose/blob/master/descriptor.json#L54C6-L65C7">here</a>) as <code class="docutils literal notranslate"><span class="pre">&quot;--diameter&quot;,</span> <span class="pre">&quot;{:f}&quot;.format(bj.parameters.diameter)</span></code>.</p>
<p>However, cellprofiler does not support changing pipeline parameters from the commandline. Maybe it will in the future.
For now, we have 3 options:</p>
<ol class="arabic simple">
<li><p>Edit the <code class="docutils literal notranslate"><span class="pre">.cppipe</span></code> file and override our parameters there automatically</p></li>
<li><p>Use the Python <code class="docutils literal notranslate"><span class="pre">cellprofiler</span></code> library in <code class="docutils literal notranslate"><span class="pre">wrapper.py</span></code> and open and edit the <code class="docutils literal notranslate"><span class="pre">pipeline</span></code>.</p></li>
<li><p>Add an extra python script that does number 2, which we call from the <code class="docutils literal notranslate"><span class="pre">wrapper.py</span></code> and which does accept commandline arguments.</p></li>
</ol>
<p>For 1., this is where <code class="docutils literal notranslate"><span class="pre">parseCPparam</span></code> <a class="reference external" href="https://github.com/Neubias-WG5/W_NucleiSegmentation-CellProfiler/blob/master/wrapper.py#L8C1-L26C1">function</a> comes in (in <code class="docutils literal notranslate"><span class="pre">wrapper.py</span></code>). I have updated it a bit in <a class="reference external" href="https://github.com/TorecLuik/W_SpotCounting-CellProfiler/blob/master/wrapper.py#L10C1-L51C24">my version</a>.
It matches the <code class="docutils literal notranslate"><span class="pre">name</span></code> in <code class="docutils literal notranslate"><span class="pre">descriptor.json</span></code> literally with the same string in <code class="docutils literal notranslate"><span class="pre">.cppipe</span></code>, and then changes the values to the new ones provided on the commandline.
However, if you use the same module twice (like in our example pipeline), it will overwrite both of them with the same value.
In our example, that does not work properly, e.g. the size of a nucleus should NOT be the same as the size of a spot.</p>
<p>Options 2 and 3 are an exercise for the reader. There is an example in the OMERO docs of using the CellProfiler Python API: <a class="reference external" href="https://omero-guides.readthedocs.io/en/latest/cellprofiler/docs/gettingstarted.html">Getting started with CellProfiler and OMERO</a>.</p>
</div>
<div class="section" id="extra-2-we-should-add-a-license">
<h2>Extra 2: We should add a LICENSE<a class="headerlink" href="#extra-2-we-should-add-a-license" title="Permalink to this headline"></a></h2>
<p>See the importance of a license <a class="reference external" href="https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/licensing-a-repository#choosing-the-right-license">here</a>:</p>
<blockquote>
<div><p>You’re under no obligation to choose a license. However, without a license, the default copyright laws apply, meaning that you retain all rights to your source code and no one may reproduce, distribute, or create derivative works from your work. If you’re creating an open source project, we strongly encourage you to include an open source license.</p>
</div></blockquote>
<p>So, we are essentially not allowed to make all these changes and use their template without a license. We will just assume we have a license as they explain all these steps in their docs.
To make this easier for the future, always add a license. I <a class="reference external" href="https://github.com/Neubias-WG5/W_NucleiSegmentation-CellProfiler/issues/2">asked</a> them to add one to the example workflows.</p>
<p>A nice permissive default is <a class="reference external" href="https://choosealicense.com/licenses/apache-2.0/">Apache 2.0</a>. It allows people to generally use it however they want, private / commercial / open / closed etc.</p>
<p>But there is also <code class="docutils literal notranslate"><span class="pre">copyleft</span></code>, where people can only adapt your code if they also keep the same license on all their code; e.g. <a class="reference external" href="https://choosealicense.com/licenses/gpl-3.0/">GNU</a>. That is a bit more restrictive.</p>
</div>
</div>
<hr class="docutils" />
<div class="section" id="cellexpansion-tutorial">
<h1>CellExpansion tutorial<a class="headerlink" href="#cellexpansion-tutorial" title="Permalink to this headline"></a></h1>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline"></a></h2>
<p>Different type of aggregates of proteins can form inside a nucleus or inside the cytoplasm of a cell.
In our example, we have aggregates (spots) outside of the nucleus and we want to quantify these per cell.</p>
</div>
<div class="section" id="import-data-to-omero">
<h2>1. Import data to OMERO<a class="headerlink" href="#import-data-to-omero" title="Permalink to this headline"></a></h2>
<p>Import <a class="reference internal" href="#./images/Cells.tif"><span class="xref myst">data</span></a> as you would normally.</p>
<p>We use this image ‘Cells.tif’, shown as part of this png with a mask here:</p>
<p><img alt="Nuclei label image" src="https://github.com/NL-BioImaging/omero-slurm-client/blob/502dd074e995b29d5206056d0f9c6eae0a3450b4/resources/tutorials/images/nuclei_labels.png?raw=true" /></p>
</div>
<div class="section" id="extract-masks-with-cellpose">
<h2>2. Extract masks with Cellpose<a class="headerlink" href="#extract-masks-with-cellpose" title="Permalink to this headline"></a></h2>
<p>This process is actually 2 steps: we want the nuclei masks and also the aggregates masks.
Luckily these were stained with different colors and are available in different channels:</p>
<ul class="simple">
<li><p>Channel 3 = Nuclei</p></li>
<li><p>Channel 2 = Aggregates</p></li>
</ul>
<p>So we can run 2 CellPose workflows on OMERO and retrieve both masks.
We store them as images in a new dataset and particularly name them: “{original_file}NucleiLabels.{ext}”  and “{original_file}GranulesLabels.{ext}”.</p>
<p>Combine both in the same dataset afterward, this will be our input dataset for the CellExpansion algorithm.</p>
</div>
<div class="section" id="cellexpansion">
<h2>3. CellExpansion<a class="headerlink" href="#cellexpansion" title="Permalink to this headline"></a></h2>
<p>To estimate the amount of aggregates per cell, we actually need the cytoplasm in our example. Then we can calculate overlap.</p>
<p>One could segment the cytoplasm, especially in this image (its just channel 1), but we have a Python script that does this algorithmically instead for the fun of it.</p>
<p>We apply the CellExpansion algorithm on the nuclei mask and estimate the full reach of the cells with new masks.</p>
<p><img alt="4 images showing cell expansion" src="https://github.com/NL-BioImaging/omero-slurm-client/blob/502dd074e995b29d5206056d0f9c6eae0a3450b4/resources/tutorials/images/cellexpansion.png?raw=true" /></p>
<p>For this, we have to first add it to OMERO:
We could just add the Python code to a OMERO job script. But then the Processor needs to have the right Python libraries installed.
Instead, we should package it in a lightweight container with the correct Python environment. This in turn makes the workflow more FAIR.</p>
<ol class="arabic simple">
<li><p>I made this workflow container for it: <a class="reference external" href="https://github.com/TorecLuik/W_CellExpansion">github repo</a>.</p></li>
<li><p>Release a version and publish a <a class="reference external" href="https://hub.docker.com/layers/torecluik/w_cellexpansion/v1.0.1/images/sha256-8d2f9e663614588f11f41c09375568b448b6d158478a968dac23dbbd8d7fdebc?context=explore">docker image</a></p></li>
<li><p>Add the workflow to Slurm and OMERO:</p></li>
</ol>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="c1"># -------------------------------------</span>
<span class="c1"># CELLEXPANSION SPOT COUNTING</span>
<span class="c1"># -------------------------------------</span>
<span class="c1"># The path to store the container on the slurm_images_path</span>
<span class="na">cellexpansion</span><span class="o">=</span><span class="s">cellexpansion</span>
<span class="c1"># The (e.g. github) repository with the descriptor.json file</span>
<span class="na">cellexpansion_repo</span><span class="o">=</span><span class="s">https://github.com/TorecLuik/W_CellExpansion/tree/v1.0.1</span>
<span class="c1"># The jobscript in the &#39;slurm_script_repo&#39;</span>
<span class="na">cellexpansion_job</span><span class="o">=</span><span class="s">jobs/cellexpansion.sh</span>
</pre></div>
</div>
<ol class="arabic simple" start="4">
<li><p>Run the workflow on our Nuclei mask.
Output the new mask back as image in a new dataset.</p></li>
</ol>
</div>
<div class="section" id="calculate-overlap">
<h2>Calculate overlap<a class="headerlink" href="#calculate-overlap" title="Permalink to this headline"></a></h2>
<p>We calculate overlap with another very short Python script.
It outputs the overlap counts of 2 masks.</p>
<p>Example original code:</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">imCellsCellLabels</span><span class="o">=</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;images/CellsNucleiLabels.tif&#39;</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">IMREAD_ANYDEPTH</span><span class="p">)</span>
<span class="n">imCellsGranulesLabels</span><span class="o">=</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;images/CellsGranulesLabels.tif&#39;</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">IMREAD_ANYDEPTH</span><span class="p">)</span>
<span class="n">numCells</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">imCellsCellLabels</span><span class="p">)</span>
<span class="n">CellNumGranules</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">numCells</span><span class="p">,</span><span class="mi">2</span><span class="p">],</span><span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int16</span><span class="p">)</span>
<span class="n">granulesStats</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">measure</span><span class="o">.</span><span class="n">regionprops_table</span><span class="p">(</span><span class="n">imCellsGranulesLabels</span><span class="p">,</span> <span class="n">properties</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;centroid&#39;</span><span class="p">,)))</span>
<span class="n">granulesStatsnp</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">granulesStats</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()),</span><span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">uint16</span><span class="p">)</span>
<span class="n">granulesStatsInCellLabel</span><span class="o">=</span><span class="n">imCellsCellLabels</span><span class="p">[</span><span class="n">granulesStatsnp</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">granulesStatsnp</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="n">numCells</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">CellNumGranules</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">count_nonzero</span><span class="p">(</span><span class="n">granulesStatsInCellLabel</span><span class="o">==</span><span class="n">i</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">CellNumGranules</span><span class="p">,</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Estimated&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">style</span>
</pre></div>
</div>
<p>I added this as a separate workflow at <a class="reference external" href="https://github.com/TorecLuik/W_CountMaskOverlap">W_CountMaskOverlap</a>.</p>
<ol class="arabic simple">
<li><p>add the workflow to config.</p></li>
<li><p>make one dataset with pairs of our mask files. We name them the same as the original image, but with an extra suffix. E.g. Cells_CellExpansion.tif and Cells_Aggregates.tif.</p></li>
<li><p>Call the new workflow on this dataset / image selection, and supply the suffixes chosen (“_CellExpansion” and “_Aggregates”) as parameter. Then make sure to upload the result of the workflow as a zip, as it will be a csv file.</p></li>
<li><p>Check the resulting csv for a count of aggregates per cell!</p></li>
</ol>
</div>
<div class="section" id="workflow-management">
<h2>Workflow management?<a class="headerlink" href="#workflow-management" title="Permalink to this headline"></a></h2>
<p>Of course, this required knowledge and manual manipulation of renaming images and supplying that metadata to the next workflow. Ideally you would be able to string singular workflows together with Input/Output like using NextFlow or Snakemake. We are looking into it for a future version.</p>
</div>
<div class="section" id="extra">
<h2>Extra<a class="headerlink" href="#extra" title="Permalink to this headline"></a></h2>
<div class="section" id="out-of-memory">
<h3>Out of memory<a class="headerlink" href="#out-of-memory" title="Permalink to this headline"></a></h3>
<p>While running CellPose on the Aggregates, my job ran out of memory. So I had to bump up the default memory used by the generated job scripts, in <code class="docutils literal notranslate"><span class="pre">slurm_config.ini</span></code>:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="c1"># -------------------------------------</span>
<span class="c1"># CELLPOSE SEGMENTATION</span>
<span class="c1"># -------------------------------------</span>
<span class="c1"># The path to store the container on the slurm_images_path</span>
<span class="na">cellpose</span><span class="o">=</span><span class="s">cellpose</span>
<span class="c1"># The (e.g. github) repository with the descriptor.json file</span>
<span class="na">cellpose_repo</span><span class="o">=</span><span class="s">https://github.com/TorecLuik/W_NucleiSegmentation-Cellpose/tree/v1.2.7</span>
<span class="c1"># The jobscript in the &#39;slurm_script_repo&#39;</span>
<span class="na">cellpose_job</span><span class="o">=</span><span class="s">jobs/cellpose.sh</span>
<span class="c1"># Override the default job values for this workflow</span>
<span class="c1"># Or add a job value to this workflow</span>
<span class="c1"># If you don&#39;t want to override, comment out / delete the line.</span>
<span class="c1"># Run CellPose Slurm with 10 GB GPU</span>
<span class="na">cellpose_job_gres</span><span class="o">=</span><span class="s">gpu:1g.10gb:1</span>
<span class="c1"># Run CellPose Slurm with 15 GB CPU memory</span>
<span class="na">cellpose_job_mem</span><span class="o">=</span><span class="s">15GB</span>
</pre></div>
</div>
<p>I added the <code class="docutils literal notranslate"><span class="pre">...mem=15GB</span></code> configuration, which will add <code class="docutils literal notranslate"><span class="pre">mem=15GB</span></code> to the Slurm job command from now on for CellPose workflows.
No need to restart the server, these changes get picked up whenever we start a new client from this config file (which is when we start a new script).</p>
<p>So after updating that <code class="docutils literal notranslate"><span class="pre">ini</span></code> file, I kickstart the workflow for channel 2 again and this time it works and returns the mask.</p>
</div>
</div>
</div>
<hr class="docutils" />
<div class="section" id="local-slurm-tutorial">
<h1>Local Slurm tutorial<a class="headerlink" href="#local-slurm-tutorial" title="Permalink to this headline"></a></h1>
<div class="section" id="id1">
<h2>Introduction<a class="headerlink" href="#id1" title="Permalink to this headline"></a></h2>
<p>This library is meant to be used with some external HPC cluster using Slurm, to offload your (OMERO) compute to servers suited for it.</p>
<p>However, if you don’t have ready access (yet) to such a cluster, you might want to spin some test environment up locally and connect your (local) OMERO to it.
This is what we will cover in this tutorial.</p>
</div>
<div class="section" id="requirements">
<h2>0. Requirements<a class="headerlink" href="#requirements" title="Permalink to this headline"></a></h2>
<p>To follow this tutorial, you need:</p>
<ul class="simple">
<li><p>Git</p></li>
<li><p>Docker (Desktop for Windows)</p></li>
<li><p>OMERO Insight</p></li>
<li><p>&gt; 18GB memory</p></li>
<li><p>&gt; 8 CPU cores</p></li>
</ul>
<p><strong>Warning</strong>: I tested with Windows here, and I’ve heard a few issues with (command-line) Linux:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">host.docker.internal</span></code> address does not work to communicate via the host machine on (command-line) Linux.</p></li>
<li><p>If you don’t run Docker as root, it won’t have access to the mounted SSH keys because of file rights.</p>
<ul class="simple">
<li><p>As an example, we run a setup on (rootless) Podman where we add SSH keys as (podman) secrets instead.</p></li>
</ul>
</li>
</ol>
<p>System requirements could be less, but then you have to change some configurations for Slurm.</p>
<p>I provide ready-to-go TL;DR, but in the details of each chapter I walk through the steps I took to make these containers ready.</p>
</div>
<div class="section" id="setup-docker-containers-for-slurm">
<h2>1. Setup Docker containers for Slurm<a class="headerlink" href="#setup-docker-containers-for-slurm" title="Permalink to this headline"></a></h2>
<div class="section" id="tl-dr">
<h3>TL;DR:<a class="headerlink" href="#tl-dr" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Clone my example <code class="docutils literal notranslate"><span class="pre">slurm-docker-cluster</span></code> locally: <a class="reference external" href="https://github.com/TorecLuik/slurm-docker-cluster">here</a></p></li>
</ul>
<details>
  <summary>Details</summary>
<p>Always a good idea to stand on the shoulders of giants, so we want to spin up a ready-made Slurm container cluster. <a class="reference external" href="https://github.com/giovtorres/slurm-docker-cluster">Here on Github</a> is a nice example with a open source license. It uses <a class="reference external" href="https://www.docker.com/">Docker</a> containers and <a class="reference external" href="https://docs.docker.com/compose/">Docker Compose</a> to easily orchestrate their interactions.</p>
<p>This setup will spin up a few separate containers (on the same computer) to make 1 slurm cluster:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">slurmdbd</span></code>, the Slurm DataBase Daemon</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">slurmctld</span></code>, the Slurm Control Daemon, our entrypoint</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mysql</span></code>, the actual database</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">c1</span></code> and <code class="docutils literal notranslate"><span class="pre">c2</span></code>, 2 compute nodes</p></li>
</ul>
<p>Note: these compute nodes are not setup to use GPU, that is a whole other challenge that we will not get into. But even on CPU, Slurm can be useful for parallel processing and keeping track of a queue of jobs.</p>
<p>So let’s clone this <a class="reference external" href="https://github.com/giovtorres/slurm-docker-cluster">repository</a> to our local system:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>git clone https://github.com/giovtorres/slurm-docker-cluster.git .
</pre></div>
</div>
<p>You can build and run these containers as described in their <a class="reference external" href="https://github.com/giovtorres/slurm-docker-cluster/blob/master/README.md">README</a>. Then you can already play around with Slurm that way, so try it out!</p>
</details>
<p>However, we are missing an ingredient: SSH access!</p>
</div>
</div>
<div class="section" id="add-ssh-access">
<h2>2. Add SSH access<a class="headerlink" href="#add-ssh-access" title="Permalink to this headline"></a></h2>
<div class="section" id="id2">
<h3>TL;DR:<a class="headerlink" href="#id2" title="Permalink to this headline"></a></h3>
<ol class="arabic simple">
<li><p>Copy your public SSH key (<code class="docutils literal notranslate"><span class="pre">id_rsa.pub</span></code>) into this git folder (it will get copied into the Docker image when you build it)</p></li>
<li><p>Add a SSH config file, store it as <code class="docutils literal notranslate"><span class="pre">~/.ssh/config</span></code> (no extension):</p></li>
</ol>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span>Host localslurm
	HostName host.docker.internal
	User slurm
	Port 2222
	IdentityFile ~/.ssh/id_rsa
	StrictHostKeyChecking no
</pre></div>
</div>
<details>
  <summary>Details</summary>
<p>We need to setup our library with SSH access between OMERO and Slurm, but this is not built-in to these containers yet (because Docker actually has a built-in alternative, <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">exec</span></code>).</p>
<p>Luckily, people have already worked on SSH access into containers too, like <a class="reference external" href="https://goteleport.com/blog/shell-access-docker-container-with-ssh-and-docker-exec/">here</a>. So let’s borrow their OpenSSH setup and add it to the <em>Dockerfile</em> of the Slurm Control Daemon (<code class="docutils literal notranslate"><span class="pre">slurmctld</span></code>):</p>
<p>======= 2a. Make a new Dockerfile for the slurmctld =======</p>
<p>We want to combine the 2 Dockerfiles. However, one is <code class="docutils literal notranslate"><span class="pre">ubuntu</span></code> and the other is <code class="docutils literal notranslate"><span class="pre">rockylinux</span></code>. The biggest difference is that <code class="docutils literal notranslate"><span class="pre">rockylinux</span></code> uses the <code class="docutils literal notranslate"><span class="pre">yum</span></code> package manager to install software, instead of <code class="docutils literal notranslate"><span class="pre">apt</span></code>. We will stick to the Slurm image as the base image and just add the OpenSSH on top of it.</p>
<p>Turns out, another difference is the use of <code class="docutils literal notranslate"><span class="pre">systemctld</span></code> causing all kinds of issues.
So I spent the time to activate OpenSSH server on Rocky linux:</p>
<div class="highlight-dockerfile notranslate"><div class="highlight"><pre><span></span><span class="k">FROM</span> <span class="s">rockylinux:8</span>

... # all the Slurm stuff from original Dockerfile ... 

<span class="c">## ------- Setup SSH ------</span>
<span class="k">RUN</span> yum update <span class="o">&amp;&amp;</span> yum install  openssh-server initscripts sudo -y
<span class="c"># Create a user “sshuser” and group “sshgroup”</span>
<span class="c"># RUN groupadd sshgroup &amp;&amp; useradd -ms /bin/bash -g sshgroup sshuser</span>
<span class="c"># Create sshuser directory in home</span>
<span class="k">RUN</span> mkdir -p /home/slurm/.ssh
<span class="c"># Copy the ssh public key in the authorized_keys file. The idkey.pub below is a public key file you get from ssh-keygen. They are under ~/.ssh directory by default.</span>
<span class="k">COPY</span> id_rsa.pub /home/slurm/.ssh/authorized_keys
<span class="c"># change ownership of the key file. </span>
<span class="k">RUN</span> chown slurm:slurm /home/slurm/.ssh/authorized_keys <span class="o">&amp;&amp;</span> chmod <span class="m">600</span> /home/slurm/.ssh/authorized_keys
<span class="c"># Start SSH service</span>
<span class="c"># RUN service ssh start</span>
<span class="c"># RUN /etc/init.d/sshd start</span>
<span class="k">RUN</span> /usr/bin/ssh-keygen -A
<span class="c"># Expose docker port 22</span>
<span class="k">EXPOSE</span><span class="s"> 22</span>
<span class="k">CMD</span> <span class="p">[</span><span class="s2">&quot;/usr/sbin/sshd&quot;</span><span class="p">,</span><span class="s2">&quot;-D&quot;</span><span class="p">]</span>
<span class="c"># CMD [&quot;slurmdbd&quot;]</span>
</pre></div>
</div>
<p>We have replaced the <code class="docutils literal notranslate"><span class="pre">slurmdbd</span></code> command (CMD) with our setup from <code class="docutils literal notranslate"><span class="pre">sshdocker</span></code>, starting a ssh daemon (<code class="docutils literal notranslate"><span class="pre">sshd</span></code>) with our SSH public key associated to the <code class="docutils literal notranslate"><span class="pre">slurm</span></code> user
.
This last part is important: to build this new version, you need to copy your public SSH key into this Docker image.
This is performed in this line:</p>
<div class="highlight-dockerfile notranslate"><div class="highlight"><pre><span></span><span class="c"># Copy the ssh public key in the authorized_keys file. The idkey.pub below is a public key file you get from ssh-keygen. They are under ~/.ssh directory by default.</span>
<span class="k">COPY</span> id_rsa.pub /home/&lt;user&gt;/.ssh/authorized_keys
</pre></div>
</div>
<p>So, you need to add your <code class="docutils literal notranslate"><span class="pre">id_rsa.pub</span></code> public key to this directory, so Docker can copy it when it builds the image.</p>
<p>Turns out, we also need to change the entrypoint script:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>... <span class="c1"># other stuff from script</span>

<span class="k">if</span> <span class="o">[</span> <span class="s2">&quot;</span><span class="nv">$1</span><span class="s2">&quot;</span> <span class="o">=</span> <span class="s2">&quot;slurmctld&quot;</span> <span class="o">]</span>
<span class="k">then</span>
    <span class="nb">echo</span> <span class="s2">&quot;---&gt; Starting the MUNGE Authentication service (munged) ...&quot;</span>
    gosu munge /usr/sbin/munged

    <span class="nb">echo</span> <span class="s2">&quot;---&gt; Starting SSH Daemon (sshd) ...&quot;</span>
    <span class="c1"># exec /usr/bin/ssh-keygen -A</span>
    <span class="nb">exec</span> /usr/sbin/sshd -D <span class="p">&amp;</span>
    <span class="nb">exec</span> rm /run/nologin <span class="p">&amp;</span>
    <span class="nb">exec</span> chmod <span class="m">777</span> /data <span class="p">&amp;</span>

    <span class="nb">echo</span> <span class="s2">&quot;---&gt; Waiting for slurmdbd to become active before starting slurmctld ...&quot;</span>

    ... <span class="c1"># other stuff from script</span>
</pre></div>
</div>
<p>We added the command to start the SSH daemon on the CTLD here, where it is actually called.
We also added some quick bugfixes to make the tutorial SSH work.
If you still run into issues with permissions in <code class="docutils literal notranslate"><span class="pre">/data</span></code>, login as superuser and also apply write access again.</p>
<p>======= 2b. Tell Docker Compose to use the new Dockerfile for <code class="docutils literal notranslate"><span class="pre">slurmctld</span></code> =======</p>
<p>Currently, <a class="reference external" href="https://github.com/giovtorres/slurm-docker-cluster/blob/master/docker-compose.yml">Docker Compose</a> will spin up all containers from the same Dockerfile definition.</p>
<p>So we will change the Dockerfile for the <code class="docutils literal notranslate"><span class="pre">slurmctld</span></code> as defined in the <code class="docutils literal notranslate"><span class="pre">docker-compose.yml</span></code>, by replacing <code class="docutils literal notranslate"><span class="pre">image</span></code> with <code class="docutils literal notranslate"><span class="pre">build</span></code>:</p>
<div class="highlight-yml notranslate"><div class="highlight"><pre><span></span>slurmctld:
    # image: slurm-docker-cluster:${IMAGE_TAG:-21.08.6}
    # Build this image from current folder
    # Use a specific file: Dockerfile_slurmctld
    build: 
      context: ./
      dockerfile: Dockerfile_slurmctld
    command: [&quot;slurmctld&quot;]
    container_name: slurmctld
    hostname: slurmctld
    volumes:
      - etc_munge:/etc/munge
      - etc_slurm:/etc/slurm
      - slurm_jobdir:/data
      - var_log_slurm:/var/log/slurm
    expose:
      - &quot;6817&quot;
    ports:
      - &quot;2222:22&quot;
    depends_on:
      - &quot;slurmdbd&quot;
</pre></div>
</div>
<p>We also mapped port 22 (SSH) from the container to our localhost port 2222.
So now we can connect SSH to our localhost and be forwarded to this Slurm container.</p>
<p>Test it out:</p>
<ol class="arabic simple">
<li><p>Fire up the Slurm cluster:</p></li>
</ol>
<div class="highlight-powershell notranslate"><div class="highlight"><pre><span></span><span class="n">docker-compose</span> <span class="n">up</span> <span class="n">-d</span> <span class="p">-</span><span class="n">-build</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li><p>SSH into the control node:</p></li>
</ol>
<div class="highlight-powershell notranslate"><div class="highlight"><pre><span></span>ssh -i C:\Users\&lt;you&gt;\.ssh\id_rsa slurm@localhost -p 2222 -o UserKnownHostsFile=/dev/null
</pre></div>
</div>
<p>This should connect as the <code class="docutils literal notranslate"><span class="pre">slurm</span></code> user to the control container on port 2222 (type yes to connect, we will fix promptless login later).</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>Last login: Tue Aug  <span class="m">8</span> <span class="m">15</span>:48:31 <span class="m">2023</span> from <span class="m">172</span>.21.0.1
<span class="o">[</span>slurm@slurmctld ~<span class="o">]</span>$
</pre></div>
</div>
<p>Congratulations!</p>
<p>======= 2c. Add SSH config for simple login =======</p>
<p>But, we can simplify the SSH, and our library needs a simple way to login.</p>
<p>For this, add <a class="reference internal" href="#../example.config"><span class="xref myst">this</span></a> config file as your <code class="docutils literal notranslate"><span class="pre">~/.ssh/config</span></code>, no extension. See <a class="reference external" href="https://www.ssh.com/academy/ssh/config">here</a> for more information.</p>
<p>Of course, first update the values with those you used to SSH before, e.g.:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="na">Host slurm</span>
	<span class="na">HostName localhost</span>
	<span class="na">User slurm</span>
	<span class="na">Port 2222</span>
	<span class="na">IdentityFile ~/.ssh/id_rsa</span>
	<span class="na">StrictHostKeyChecking no</span>
</pre></div>
</div>
<p>Then try it out:
<code class="docutils literal notranslate"><span class="pre">ssh</span> <span class="pre">slurm</span></code></p>
<p>======= StrictHostKeyChecking =======
Note that I added <code class="docutils literal notranslate"><span class="pre">StrictHostKeyChecking</span> <span class="pre">no</span></code>, as our Slurm container will have different keys all the time. A normal Slurm server likely does not, and won’t require this flag. This is also where we get our pretty warning from:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>...&gt; ssh slurm
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!
Someone could be eavesdropping on you right now (man-in-the-middle attack)!
It is also possible that a host key has just been changed.
</pre></div>
</div>
<p>The host key changed =)</p>
<p>If you don’t add this flag, it will safe you from danger and deny access. Of course, that is not very useful for our tutorial.</p>
</details>
</div>
</div>
<div class="section" id="test-slurm">
<h2>3. Test Slurm<a class="headerlink" href="#test-slurm" title="Permalink to this headline"></a></h2>
<div class="section" id="id3">
<h3>TL;DR:<a class="headerlink" href="#id3" title="Permalink to this headline"></a></h3>
<ol class="arabic simple">
<li><p>Spin up the Slurm cluster: <code class="docutils literal notranslate"><span class="pre">docker-compose</span> <span class="pre">up</span> <span class="pre">-d</span> <span class="pre">--build</span></code></p></li>
<li><p>SSH into the control node: <code class="docutils literal notranslate"><span class="pre">ssh</span> <span class="pre">localslurm</span></code></p></li>
<li><p>Start some filler jobs: <code class="docutils literal notranslate"><span class="pre">sbatch</span> <span class="pre">--wrap=&quot;sleep</span> <span class="pre">5</span> <span class="pre">&amp;&amp;</span> <span class="pre">hostname&quot;</span> <span class="pre">&amp;&amp;</span>&#160; <span class="pre">sbatch</span> <span class="pre">--wrap=&quot;sleep</span> <span class="pre">5</span> <span class="pre">&amp;&amp;</span> <span class="pre">hostname&quot;</span> <span class="pre">&amp;&amp;</span>&#160; <span class="pre">sbatch</span> <span class="pre">--wrap=&quot;sleep</span> <span class="pre">5</span> <span class="pre">&amp;&amp;</span> <span class="pre">hostname&quot;</span> <span class="pre">&amp;&amp;</span>&#160; <span class="pre">sbatch</span> <span class="pre">--wrap=&quot;sleep</span> <span class="pre">5</span> <span class="pre">&amp;&amp;</span> <span class="pre">hostname&quot;</span></code></p></li>
<li><p>Check the progress: <code class="docutils literal notranslate"><span class="pre">squeue</span></code></p></li>
<li><p>Check some output, e.g. job 1: <code class="docutils literal notranslate"><span class="pre">cat</span> <span class="pre">slurm-1.out</span></code></p></li>
</ol>
<details>
  <summary>Details</summary>
<p>Now connect via SSH to Slurm, change to <code class="docutils literal notranslate"><span class="pre">/data</span></code> (our fileserver shared between the Slurm nodes) and let’s see if Slurm works:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>slurm@slurmctld ~<span class="o">]</span>$ <span class="nb">cd</span> /data
<span class="o">[</span>slurm@slurmctld data<span class="o">]</span>$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST<span class="o">(</span>REASON<span class="o">)</span>
<span class="o">[</span>slurm@slurmctld data<span class="o">]</span>$
</pre></div>
</div>
<p>The queue is empty!
Let’s fill it up with some short tasks:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>slurm@slurmctld data<span class="o">]</span>$ sbatch --wrap<span class="o">=</span><span class="s2">&quot;sleep 5 &amp;&amp; hostname&quot;</span> <span class="o">&amp;&amp;</span>  sbatch --wrap<span class="o">=</span><span class="s2">&quot;sleep 5 &amp;&amp; hostname&quot;</span> <span class="o">&amp;&amp;</span>  sbatch --wrap<span class="o">=</span><span class="s2">&quot;sleep 5 &amp;&amp; hostname&quot;</span> <span class="o">&amp;&amp;</span>  sbatch --wrap<span class="o">=</span><span class="s2">&quot;sleep 5 &amp;&amp; hostname&quot;</span>
Submitted batch job <span class="m">5</span>
Submitted batch job <span class="m">6</span>
Submitted batch job <span class="m">7</span>
Submitted batch job <span class="m">8</span>
<span class="o">[</span>slurm@slurmctld data<span class="o">]</span>$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST<span class="o">(</span>REASON<span class="o">)</span>
                 <span class="m">7</span>    normal     wrap    slurm  R       <span class="m">0</span>:01      <span class="m">1</span> c1
                 <span class="m">8</span>    normal     wrap    slurm  R       <span class="m">0</span>:01      <span class="m">1</span> c2
<span class="o">[</span>slurm@slurmctld data<span class="o">]</span>$
</pre></div>
</div>
<p>I fired off 4 jobs that take 2 seconds, so a few remained in the queue by the time I called for an update. You can also see they split over the 2 compute nodes <code class="docutils literal notranslate"><span class="pre">c1</span></code> and <code class="docutils literal notranslate"><span class="pre">c2</span></code>.</p>
<p>The jobs wrote their stdout output in the current dir (<code class="docutils literal notranslate"><span class="pre">/data</span></code>, which is where permission issues might come in):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>slurm@slurmctld data<span class="o">]</span>$ ls
slurm-3.out  slurm-4.out  slurm-5.out  slurm-6.out  slurm-7.out  slurm-8.out
<span class="o">[</span>slurm@slurmctld data<span class="o">]</span>$ cat slurm-7.out
c1
<span class="o">[</span>slurm@slurmctld data<span class="o">]</span>$ cat slurm-8.out
c2
<span class="o">[</span>slurm@slurmctld data<span class="o">]</span>$
</pre></div>
</div>
</details>
<p>They logged the <code class="docutils literal notranslate"><span class="pre">hostname</span></code> command, which returned <code class="docutils literal notranslate"><span class="pre">c1</span></code> for some and <code class="docutils literal notranslate"><span class="pre">c2</span></code> for others, as those were the hosts the compute was used from.</p>
<p>Now let’s connect OMERO to our Slurm!</p>
</div>
</div>
<div class="section" id="omero-omero-slurm-client">
<h2>4. OMERO &amp; OMERO Slurm Client<a class="headerlink" href="#omero-omero-slurm-client" title="Permalink to this headline"></a></h2>
<p>Ok, now we need a OMERO server and a correctly configured OMERO Slurm Client.</p>
<div class="section" id="id4">
<h3>TL;DR:<a class="headerlink" href="#id4" title="Permalink to this headline"></a></h3>
<ol class="arabic simple">
<li><p>Clone my example <code class="docutils literal notranslate"><span class="pre">docker-example-omero-grid-amc</span></code> locally: <code class="docutils literal notranslate"><span class="pre">git</span> <span class="pre">clone</span> <span class="pre">-b</span> <span class="pre">processors</span> <span class="pre">https://github.com/TorecLuik/docker-example-omero-grid-amc.git</span></code></p></li>
<li><p>Fire up the OMERO containers: <code class="docutils literal notranslate"><span class="pre">docker-compose</span> <span class="pre">up</span> <span class="pre">-d</span> <span class="pre">--build</span></code></p></li>
<li><p>Go to OMERO.web (<code class="docutils literal notranslate"><span class="pre">localhost:4080</span></code>), login <code class="docutils literal notranslate"><span class="pre">root</span></code> pw <code class="docutils literal notranslate"><span class="pre">omero</span></code></p></li>
<li><p>Upload some images (to <code class="docutils literal notranslate"><span class="pre">localhost</span></code>) with OMERO.Insight (e.g. <a class="reference external" href="https://github.com/NL-BioImaging/omero-slurm-client/blob/main/resources/tutorials/images/Cells.tif">Cells.tiff</a>).</p></li>
<li><p>In web, run the <code class="docutils literal notranslate"><span class="pre">slurm/init_environment</span></code> script (<a class="reference external" href="https://github.com/NL-BioImaging/omero-slurm-scripts/blob/master/init/SLURM_Init_environment.py">here</a>)</p></li>
</ol>
<details>
  <summary>Details</summary>
<p>======= OMERO in Docker =======</p>
<p>You can use your own OMERO setup, but for this tutorial I will refer to a dockerized OMERO that I am working with: <a class="reference external" href="https://github.com/TorecLuik/docker-example-omero-grid-amc/tree/processors">get it here</a>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git clone -b processors https://github.com/TorecLuik/docker-example-omero-grid-amc.git
</pre></div>
</div>
<p>Let’s (build it and) fire it up:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker-compose up -d --build
</pre></div>
</div>
<p>======= OMERO web =======</p>
<p>Once they are running, you should be able to access web at <code class="docutils literal notranslate"><span class="pre">localhost:4080</span></code>. Login with user <code class="docutils literal notranslate"><span class="pre">root</span></code> / pw <code class="docutils literal notranslate"><span class="pre">omero</span></code>.</p>
<p>Import some example data with OMERO Insight (connect with <code class="docutils literal notranslate"><span class="pre">localhost</span></code>).</p>
<p>======= Connect to Slurm =======</p>
<p>This container’s processor node (<code class="docutils literal notranslate"><span class="pre">worker-5</span></code>) has already installed our <code class="docutils literal notranslate"><span class="pre">omero-slurm-client</span></code> library.</p>
<p>======= Add ssh config to OMERO Processor =======</p>
<p>Ok, so <code class="docutils literal notranslate"><span class="pre">localhost</span></code> works fine from your machine, but we need the OMERO processing server <code class="docutils literal notranslate"><span class="pre">worker-5</span></code> to be able to do it too, like <a class="reference internal" href="#2c-add-ssh-config-for-simple-login"><span class="xref myst">we did before</span></a>.</p>
<p>By some smart tricks, we have mounted our <code class="docutils literal notranslate"><span class="pre">~/.ssh</span></code> folder to the worker container, so it knows and can use our SSH settings and config.</p>
<p>However, we need to change the <code class="docutils literal notranslate"><span class="pre">HostName</span></code> to match one that the container can understand. <code class="docutils literal notranslate"><span class="pre">localhost</span></code> works fine from our machine, but not from within a Docker container. Instead, we need to use <code class="docutils literal notranslate"><span class="pre">host.docker.internal</span></code> (<a class="reference external" href="https://docs.docker.com/desktop/networking/#i-want-to-connect-from-a-container-to-a-service-on-the-host">documentation</a>).</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="na">Host slurm</span>
	<span class="na">HostName host.docker.internal</span>
	<span class="na">User slurm</span>
	<span class="na">Port 2222</span>
	<span class="na">IdentityFile ~/.ssh/id_rsa</span>
	<span class="na">StrictHostKeyChecking no</span>
</pre></div>
</div>
<p>Restart your OMERO cluster if you already started it:
<code class="docutils literal notranslate"><span class="pre">docker-compose</span> <span class="pre">down</span></code> &amp; <code class="docutils literal notranslate"><span class="pre">docker-compose</span> <span class="pre">up</span> <span class="pre">-d</span> <span class="pre">--build</span></code></p>
<p>Ok, so now we can connect from within the worker-5 to our Slurm cluster. We can try it out:</p>
<div class="highlight-powershell notranslate"><div class="highlight"><pre><span></span>...\docker-example-omero-grid&gt; docker-compose exec omeroworker-5 /bin/bash
bash-4.2$ ssh slurm
Last login: Wed Aug  9 13:08:54 2023 from 172.21.0.1
[slurm@slurmctld ~]$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
[slurm@slurmctld ~]$ exit
logout
Connection to host.docker.internal closed.
bash-4.2$ exit
exit
</pre></div>
</div>
<p>======= slurm-config.ini =======</p>
<p>Let us setup the library’s config file <a class="reference internal" href="#../slurm-config.ini"><span class="xref myst">slurm-config.ini</span></a> correctly.</p>
<p>Now, the <code class="docutils literal notranslate"><span class="pre">omero-slurm-client</span></code> library by default expects the <code class="docutils literal notranslate"><span class="pre">Slurm</span></code> ssh connection to be called <code class="docutils literal notranslate"><span class="pre">slurm</span></code>, but you can adjust it to whatever you named your ssh <em>Host</em> in config.</p>
<p>In this Docker setup, the config file is located at the <code class="docutils literal notranslate"><span class="pre">worker-gpu</span></code> folder and in the Dockerfile it is copied to <code class="docutils literal notranslate"><span class="pre">/etc/</span></code>, where the library will pick it up.</p>
<p>Let’s use these values:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[SSH]</span>
<span class="c1"># -------------------------------------</span>
<span class="c1"># SSH settings</span>
<span class="c1"># -------------------------------------</span>
<span class="c1"># The alias for the SLURM SSH connection</span>
<span class="na">host</span><span class="o">=</span><span class="s">slurm</span>
<span class="c1"># Set the rest of your SSH configuration in your SSH config under this host name/alias</span>
<span class="c1"># Or in e.g. /etc/fabric.yml (see Fabric&#39;s documentation for details on config loading)</span>

<span class="k">[SLURM]</span>
<span class="c1"># -------------------------------------</span>
<span class="c1"># Slurm settings</span>
<span class="c1"># -------------------------------------</span>
<span class="c1"># General settings for where to find things on the Slurm cluster.</span>
<span class="c1"># -------------------------------------</span>
<span class="c1"># PATHS</span>
<span class="c1"># -------------------------------------</span>
<span class="c1"># The path on SLURM entrypoint for storing datafiles</span>
<span class="c1">#</span>
<span class="c1"># Note: </span>
<span class="c1"># This example is relative to the Slurm user&#39;s home dir</span>
<span class="na">slurm_data_path</span><span class="o">=</span><span class="s">/data/my-scratch/data</span>
<span class="c1"># The path on SLURM entrypoint for storing container image files</span>
<span class="c1">#</span>
<span class="c1"># Note: </span>
<span class="c1"># This example is relative to the Slurm user&#39;s home dir</span>
<span class="na">slurm_images_path</span><span class="o">=</span><span class="s">/data/my-scratch/singularity_images/workflows</span>
<span class="c1"># The path on SLURM entrypoint for storing the slurm job scripts</span>
<span class="c1">#</span>
<span class="c1"># Note: </span>
<span class="c1"># This example is relative to the Slurm user&#39;s home dir</span>
<span class="na">slurm_script_path</span><span class="o">=</span><span class="s">/data/my-scratch/slurm-scripts</span>
</pre></div>
</div>
<p>We have put all the storage paths on <code class="docutils literal notranslate"><span class="pre">/data/my-scratch/</span></code> and named the SSH Host connection <code class="docutils literal notranslate"><span class="pre">slurm</span></code>.</p>
<p>The other values we can keep as <a class="reference internal" href="#../slurm-config.ini"><span class="xref myst">default</span></a>, except we don’t have a GPU, so let’s turn that off for CellPose:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="c1"># -------------------------------------</span>
<span class="c1"># CELLPOSE SEGMENTATION</span>
<span class="c1"># -------------------------------------</span>
<span class="c1"># The path to store the container on the slurm_images_path</span>
<span class="na">cellpose</span><span class="o">=</span><span class="s">cellpose</span>
<span class="c1"># The (e.g. github) repository with the descriptor.json file</span>
<span class="na">cellpose_repo</span><span class="o">=</span><span class="s">https://github.com/TorecLuik/W_NucleiSegmentation-Cellpose/tree/v1.2.7</span>
<span class="c1"># The jobscript in the &#39;slurm_script_repo&#39;</span>
<span class="na">cellpose_job</span><span class="o">=</span><span class="s">jobs/cellpose.sh</span>
<span class="c1"># Override the default job values for this workflow</span>
<span class="c1"># Or add a job value to this workflow</span>
<span class="c1"># For more examples of such parameters, google SBATCH parameters.</span>
<span class="c1"># If you don&#39;t want to override, comment out / delete the line.</span>
<span class="c1"># Run CellPose Slurm with 10 GB GPU</span>
<span class="c1"># cellpose_job_gres=gpu:1g.10gb:1</span>
<span class="c1"># Run CellPose Slurm with 15 GB CPU memory</span>
<span class="na">cellpose_job_mem</span><span class="o">=</span><span class="s">15GB</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">gres</span></code> will request a 10GB GPU on the Slurm cluster, but we only set up CPU docker slurm.</p>
<p>We will also comment out some of the other algorithms, so we have to download less containers to our Slurm cluster and speed up the tutorial.</p>
<p>This brings us to the following configuration file:</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[SSH]</span>
<span class="c1"># -------------------------------------</span>
<span class="c1"># SSH settings</span>
<span class="c1"># -------------------------------------</span>
<span class="c1"># The alias for the SLURM SSH connection</span>
<span class="na">host</span><span class="o">=</span><span class="s">slurm</span>
<span class="c1"># Set the rest of your SSH configuration in your SSH config under this host name/alias</span>
<span class="c1"># Or in e.g. /etc/fabric.yml (see Fabric&#39;s documentation for details on config loading)</span>

<span class="k">[SLURM]</span>
<span class="c1"># -------------------------------------</span>
<span class="c1"># Slurm settings</span>
<span class="c1"># -------------------------------------</span>
<span class="c1"># General settings for where to find things on the Slurm cluster.</span>
<span class="c1"># -------------------------------------</span>
<span class="c1"># PATHS</span>
<span class="c1"># -------------------------------------</span>
<span class="c1"># The path on SLURM entrypoint for storing datafiles</span>
<span class="c1">#</span>
<span class="c1"># Note: </span>
<span class="c1"># This example is relative to the Slurm user&#39;s home dir</span>
<span class="na">slurm_data_path</span><span class="o">=</span><span class="s">/data/my-scratch/data</span>
<span class="c1"># The path on SLURM entrypoint for storing container image files</span>
<span class="c1">#</span>
<span class="c1"># Note: </span>
<span class="c1"># This example is relative to the Slurm user&#39;s home dir</span>
<span class="na">slurm_images_path</span><span class="o">=</span><span class="s">/data/my-scratch/singularity_images/workflows</span>
<span class="c1"># The path on SLURM entrypoint for storing the slurm job scripts</span>
<span class="c1">#</span>
<span class="c1"># Note: </span>
<span class="c1"># This example is relative to the Slurm user&#39;s home dir</span>
<span class="na">slurm_script_path</span><span class="o">=</span><span class="s">/data/my-scratch/slurm-scripts</span>
<span class="c1"># -------------------------------------</span>
<span class="c1"># REPOSITORIES</span>
<span class="c1"># -------------------------------------</span>
<span class="c1"># A (github) repository to pull the slurm scripts from.</span>
<span class="c1">#</span>
<span class="c1"># Note: </span>
<span class="c1"># If you provide no repository, we will generate scripts instead!</span>
<span class="c1"># Based on the job_template and the descriptor.json</span>
<span class="c1">#</span>
<span class="c1"># Example:</span>
<span class="c1">#slurm_script_repo=https://github.com/TorecLuik/slurm-scripts</span>
<span class="na">slurm_script_repo</span><span class="o">=</span>
<span class="c1"># -------------------------------------</span>
<span class="c1"># Processing settings</span>
<span class="c1"># -------------------------------------</span>
<span class="c1"># General/default settings for processing jobs.</span>
<span class="c1"># Note: NOT YET IMPLEMENTED</span>
<span class="c1"># Note: If you need to change it for a specific case only,</span>
<span class="c1"># you should change the job script instead, either in OMERO or Slurm </span>


<span class="k">[MODELS]</span>
<span class="c1"># -------------------------------------</span>
<span class="c1"># Model settings</span>
<span class="c1"># -------------------------------------</span>
<span class="c1"># Settings for models/singularity images that we want to run on Slurm</span>
<span class="c1">#</span>
<span class="c1"># NOTE: keys have to be unique, and require a &lt;key&gt;_repo and &lt;key&gt;_image value as well.</span>
<span class="c1">#</span>
<span class="c1"># NOTE 2: Versions for the repo are highly encouraged! </span>
<span class="c1"># Latest/master can change and cause issues with reproducability!</span>
<span class="c1"># We pickup the container version based on the version of the repository.</span>
<span class="c1"># For generic master branch, we pick up generic latest container.</span>
<span class="c1"># -------------------------------------</span>
<span class="c1"># CELLPOSE SEGMENTATION</span>
<span class="c1"># -------------------------------------</span>
<span class="c1"># The path to store the container on the slurm_images_path</span>
<span class="na">cellpose</span><span class="o">=</span><span class="s">cellpose</span>
<span class="c1"># The (e.g. github) repository with the descriptor.json file</span>
<span class="na">cellpose_repo</span><span class="o">=</span><span class="s">https://github.com/TorecLuik/W_NucleiSegmentation-Cellpose/tree/v1.2.7</span>
<span class="c1"># The jobscript in the &#39;slurm_script_repo&#39;</span>
<span class="na">cellpose_job</span><span class="o">=</span><span class="s">jobs/cellpose.sh</span>
<span class="c1"># Override the default job values for this workflow</span>
<span class="c1"># Or add a job value to this workflow</span>
<span class="c1"># For more examples of such parameters, google SBATCH parameters.</span>
<span class="c1"># If you don&#39;t want to override, comment out / delete the line.</span>
<span class="c1"># Run CellPose Slurm with 10 GB GPU</span>
<span class="c1"># cellpose_job_gres=gpu:1g.10gb:1</span>
<span class="c1"># Run CellPose Slurm with 15 GB CPU memory</span>
<span class="na">cellpose_job_mem</span><span class="o">=</span><span class="s">15GB</span>
<span class="c1"># -------------------------------------</span>
<span class="c1"># # STARDIST SEGMENTATION</span>
<span class="c1"># # -------------------------------------</span>
<span class="c1"># # The path to store the container on the slurm_images_path</span>
<span class="c1"># stardist=stardist</span>
<span class="c1"># # The (e.g. github) repository with the descriptor.json file</span>
<span class="c1"># stardist_repo=https://github.com/Neubias-WG5/W_NucleiSegmentation-Stardist/tree/v1.3.2</span>
<span class="c1"># # The jobscript in the &#39;slurm_script_repo&#39;</span>
<span class="c1"># stardist_job=jobs/stardist.sh</span>
<span class="c1"># -------------------------------------</span>
<span class="c1"># CELLPROFILER SEGMENTATION</span>
<span class="c1"># # -------------------------------------</span>
<span class="c1"># # The path to store the container on the slurm_images_path</span>
<span class="c1"># cellprofiler=cellprofiler</span>
<span class="c1"># # The (e.g. github) repository with the descriptor.json file</span>
<span class="c1"># cellprofiler_repo=https://github.com/Neubias-WG5/W_NucleiSegmentation-CellProfiler/tree/v1.6.4</span>
<span class="c1"># # The jobscript in the &#39;slurm_script_repo&#39;</span>
<span class="c1"># cellprofiler_job=jobs/cellprofiler.sh</span>
<span class="c1"># -------------------------------------</span>
<span class="c1"># DEEPCELL SEGMENTATION</span>
<span class="c1"># # -------------------------------------</span>
<span class="c1"># # The path to store the container on the slurm_images_path</span>
<span class="c1"># deepcell=deepcell</span>
<span class="c1"># # The (e.g. github) repository with the descriptor.json file</span>
<span class="c1"># deepcell_repo=https://github.com/Neubias-WG5/W_NucleiSegmentation-DeepCell/tree/v.1.4.3</span>
<span class="c1"># # The jobscript in the &#39;slurm_script_repo&#39;</span>
<span class="c1"># deepcell_job=jobs/deepcell.sh</span>
<span class="c1"># -------------------------------------</span>
<span class="c1"># IMAGEJ SEGMENTATION</span>
<span class="c1"># # -------------------------------------</span>
<span class="c1"># # The path to store the container on the slurm_images_path</span>
<span class="c1"># imagej=imagej</span>
<span class="c1"># # The (e.g. github) repository with the descriptor.json file</span>
<span class="c1"># imagej_repo=https://github.com/Neubias-WG5/W_NucleiSegmentation-ImageJ/tree/v1.12.10</span>
<span class="c1"># # The jobscript in the &#39;slurm_script_repo&#39;</span>
<span class="c1"># imagej_job=jobs/imagej.sh</span>
<span class="c1"># # -------------------------------------</span>
<span class="c1"># # CELLPROFILER SPOT COUNTING</span>
<span class="c1"># # -------------------------------------</span>
<span class="c1"># The path to store the container on the slurm_images_path</span>
<span class="na">cellprofiler_spot</span><span class="o">=</span><span class="s">cellprofiler_spot</span>
<span class="c1"># The (e.g. github) repository with the descriptor.json file</span>
<span class="na">cellprofiler_spot_repo</span><span class="o">=</span><span class="s">https://github.com/TorecLuik/W_SpotCounting-CellProfiler/tree/v1.0.1</span>
<span class="c1"># The jobscript in the &#39;slurm_script_repo&#39;</span>
<span class="na">cellprofiler_spot_job</span><span class="o">=</span><span class="s">jobs/cellprofiler_spot.sh</span>
<span class="c1"># # -------------------------------------</span>
<span class="c1"># CELLEXPANSION SPOT COUNTING</span>
<span class="c1"># -------------------------------------</span>
<span class="c1"># The path to store the container on the slurm_images_path</span>
<span class="na">cellexpansion</span><span class="o">=</span><span class="s">cellexpansion</span>
<span class="c1"># The (e.g. github) repository with the descriptor.json file</span>
<span class="na">cellexpansion_repo</span><span class="o">=</span><span class="s">https://github.com/TorecLuik/W_CellExpansion/tree/v1.0.1</span>
<span class="c1"># The jobscript in the &#39;slurm_script_repo&#39;</span>
<span class="na">cellexpansion_job</span><span class="o">=</span><span class="s">jobs/cellexpansion.sh</span>
</pre></div>
</div>
<p>======= Init environment =======</p>
<p>Now we go to OMERO web and run the <code class="docutils literal notranslate"><span class="pre">slurm/init_environment</span></code> script to apply this config and setup our Slurm. We will use the default location, no need to fill in anything, just run the script.</p>
<p><img alt="Slurm Init Busy" src="images/webclient_init_env.PNG" /></p>
<p><img alt="Slurm Init Done" src="images/webclient_init_env_done.PNG" /></p>
</details>
<p>Note, this will take a while, since it is downloading workflow docker images and building (singularity) containers from them.</p>
<p>Congratulations! We have setup workflows CellPose <code class="docutils literal notranslate"><span class="pre">v1.2.7</span></code>, Cellprofiler Spot <code class="docutils literal notranslate"><span class="pre">v1.0.1</span></code> and CellExpansion <code class="docutils literal notranslate"><span class="pre">v1.0.1</span></code>. And there are no data files yet.</p>
<p>Let’s go run some segmentation workflow then!</p>
</div>
</div>
<div class="section" id="workflows">
<h2>5. Workflows!<a class="headerlink" href="#workflows" title="Permalink to this headline"></a></h2>
<div class="section" id="id5">
<h3>TL;DR:<a class="headerlink" href="#id5" title="Permalink to this headline"></a></h3>
<ol class="arabic simple">
<li><p>In web, select your images and run script <code class="docutils literal notranslate"><span class="pre">slurm/SLURM</span> <span class="pre">Run</span> <span class="pre">Workflow</span></code></p>
<ul class="simple">
<li><p>Tick off <code class="docutils literal notranslate"><span class="pre">E-mail</span></code> box (not implemented in this Slurm docker setup)</p></li>
<li><p>For importing results, change <code class="docutils literal notranslate"><span class="pre">3a)</span> <span class="pre">Import</span> <span class="pre">into</span> <span class="pre">NEW</span> <span class="pre">Dataset</span></code> to <code class="docutils literal notranslate"><span class="pre">CellPose_Masks</span></code></p></li>
<li><p>For importing results, change <code class="docutils literal notranslate"><span class="pre">3b)</span> <span class="pre">Rename</span> <span class="pre">the</span> <span class="pre">imported</span> <span class="pre">images</span></code> to <code class="docutils literal notranslate"><span class="pre">{original_file}_cpmask.{ext}</span></code></p></li>
<li><p>Select <code class="docutils literal notranslate"><span class="pre">cellpose</span></code>, but tick off <code class="docutils literal notranslate"><span class="pre">use_gpu</span></code> off (sadly not implemented in this docker setup)</p></li>
<li><p>Click <code class="docutils literal notranslate"><span class="pre">Run</span> <span class="pre">Script</span></code></p></li>
</ul>
</li>
<li><p>Check activity window (or get a coffee), it should take a few minutes (about 3m:30s for 4 256x256 images for me) and then say (a.o.): <code class="docutils literal notranslate"><span class="pre">COMPLETE</span></code></p>
<ul class="simple">
<li><p>Or it <code class="docutils literal notranslate"><span class="pre">FAILED</span></code>, in which case you should check all the details anyway and get your hands dirty with debugging! Or try less and smaller images.</p></li>
</ul>
</li>
<li><p>Refresh your Explore window, there should be a new dataset <code class="docutils literal notranslate"><span class="pre">CellPose_Masks</span></code> with a mask for every input image.</p></li>
</ol>
<details>
  <summary>Details</summary>
<p>So, I hope you added some data already; if not, import some images now.</p>
<p>Let’s run <code class="docutils literal notranslate"><span class="pre">slurm/SLURM</span> <span class="pre">Run</span> <span class="pre">Workflow</span></code>:</p>
<p><img alt="Slurm Run Workflow" src="images/webclient_run_workflow.PNG?raw=true" /></p>
<p>You can see that this script recognized that we downloaded 3 workflows, and what their parameters are. For more information on this magic, follow the other tutorials.</p>
<p>Let’s select <code class="docutils literal notranslate"><span class="pre">cellpose</span></code> and click <code class="docutils literal notranslate"><span class="pre">use</span> <span class="pre">gpu</span></code> off (sadly). Tune the other parameters as you like for your images. Also, for output let’s select <code class="docutils literal notranslate"><span class="pre">Import</span> <span class="pre">into</span> <span class="pre">NEW</span> <span class="pre">Dataset</span></code> by filling in a dataset name: cellpose_images. Click <code class="docutils literal notranslate"><span class="pre">Run</span> <span class="pre">Script</span></code>.</p>
<p><img alt="Slurm Run Cellpose" src="images/webclient_run_cellpose.PNG?raw=true" /></p>
<p>Result: Job 1 is FAILED.
Turns out, our Slurm doesn’t have the compute nodes to execute this operation.</p>
<p>======= Improve Slurm =======</p>
<p>Update the <code class="docutils literal notranslate"><span class="pre">slurm.conf</span></code> file in the git repository.</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="c1"># COMPUTE NODES</span>
<span class="na">NodeName</span><span class="o">=</span><span class="s">c[1-2] RealMemory=5120 CPUs=8 State=UNKNOWN</span>
</pre></div>
</div>
<p>Here, 5GB and 8 CPU each should do the trick!</p>
<p>Rebuild the containers. Note that the config is on a shared volume, so we have to destroy that volume too (it took some headbashing to find this out):</p>
<div class="highlight-powershell notranslate"><div class="highlight"><pre><span></span><span class="n">docker-compose</span> <span class="n">down</span> <span class="p">-</span><span class="n">-volumes</span> 
</pre></div>
</div>
<div class="highlight-powershell notranslate"><div class="highlight"><pre><span></span><span class="n">docker-compose</span> <span class="n">up</span> <span class="p">-</span><span class="n">-build</span>
</pre></div>
</div>
</details>
<p>That should take you through connecting OMERO with a local Slurm setup.</p>
</div>
<div class="section" id="batching">
<h3>Batching<a class="headerlink" href="#batching" title="Permalink to this headline"></a></h3>
<p>Try <code class="docutils literal notranslate"><span class="pre">slurm/SLURM</span> <span class="pre">Run</span> <span class="pre">Workflow</span> <span class="pre">Batched</span></code> (here)[https://github.com/NL-BioImaging/omero-slurm-scripts/blob/master/workflows/SLURM_Run_Workflow_Batched.py] to see if there is any speedup by splitting your images over multiple jobs/batches.</p>
<p>We have installed 2 nodes in this Slurm cluster, so you could make 2 batches of half the images and get your results quicker. However we are also limited to compute 2 jobs in parallel, so smaller (than half) batches will just wait in the queue (with some overhead) and probably take longer in total.</p>
<details>
Note that there is always overhead cost, so the speedup will not be linear. However, the more time is in compute vs overhead, the more gains you should get by splitting over multiple jobs / nodes / CPUs.
<p>Let’s check on the Slurm node:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ sacct --starttime <span class="s2">&quot;2023-06-13T17:00:00&quot;</span> --format Jobid,State,start,end,JobName%-18,Elapsed -n -X --endtime <span class="s2">&quot;now&quot;</span>
</pre></div>
</div>
<p>In my latest example, it was 1 minute (30%) faster to have 2 batches/jobs (<code class="docutils literal notranslate"><span class="pre">32</span></code> &amp; <code class="docutils literal notranslate"><span class="pre">33</span></code>) vs 1 job (<code class="docutils literal notranslate"><span class="pre">31</span></code>):</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">31            COMPLETED 2023-08-23T08:41:28 2023-08-23T08:45:02 omero-job-cellpose   00:03:34</span>

<span class="l l-Scalar l-Scalar-Plain">32            COMPLETED 2023-08-23T09:22:00 2023-08-23T09:24:27 omero-job-cellpose   00:02:27</span>
<span class="l l-Scalar l-Scalar-Plain">33            COMPLETED 2023-08-23T09:22:03 2023-08-23T09:24:40 omero-job-cellpose   00:02:37</span>
</pre></div>
</div>
</details>
</div>
</div>
</div>
<hr class="docutils" />
<div class="section" id="google-cloud-slurm-tutorial">
<h1>Google Cloud Slurm tutorial<a class="headerlink" href="#google-cloud-slurm-tutorial" title="Permalink to this headline"></a></h1>
<div class="section" id="id6">
<h2>Introduction<a class="headerlink" href="#id6" title="Permalink to this headline"></a></h2>
<p>This library is meant to be used with some external HPC cluster using Slurm, to offload your (OMERO) compute to servers suited for it.</p>
<p>However, if you don’t have ready access (yet) to such a cluster, you might want to spin some test environment up in the Cloud and connect your (local) OMERO to it.
This is what we will cover in this tutorial, specifically Google Cloud.</p>
</div>
<div class="section" id="id7">
<h2>0. Requirements<a class="headerlink" href="#id7" title="Permalink to this headline"></a></h2>
<p>To follow this tutorial, you need:</p>
<ul class="simple">
<li><p>Git</p></li>
<li><p>Docker</p></li>
<li><p>OMERO Insight</p></li>
<li><p>A creditcard (but we’ll work with free credits)</p></li>
</ul>
<p>I use Windows here, but it should work on Linux/Mac too. If not, let me know.</p>
<p>I provide ready-to-go TL;DR, but in the details of each chapter I walk through the steps I took to make these containers ready.</p>
</div>
<div class="section" id="setup-google-cloud-for-slurm">
<h2>1. Setup Google Cloud for Slurm<a class="headerlink" href="#setup-google-cloud-for-slurm" title="Permalink to this headline"></a></h2>
<div class="section" id="id8">
<h3>TL;DR:<a class="headerlink" href="#id8" title="Permalink to this headline"></a></h3>
<ol class="arabic simple">
<li><p>Follow this <a class="reference external" href="https://cloud.google.com/hpc-toolkit/docs/quickstarts/slurm-cluster">tutorial</a> from Google Cloud. Click ‘guide me’.</p></li>
<li><p>Make a new Google Account to do this, with free $300 credits to use Slurm for a bit. This requires the creditcard (but no cost).</p></li>
</ol>
<details>
  <summary>Details</summary>
<p>So, we follow this <a class="reference external" href="https://cloud.google.com/hpc-toolkit/docs/quickstarts/slurm-cluster">tutorial</a> and end up with a <code class="docutils literal notranslate"><span class="pre">hpcsmall</span></code> VM on Google Cloud.</p>
</details>
<p>However, we are missing an ingredient: SSH access!</p>
</div>
</div>
<div class="section" id="id9">
<h2>2. Add SSH access<a class="headerlink" href="#id9" title="Permalink to this headline"></a></h2>
<div class="section" id="id10">
<h3>TL;DR:<a class="headerlink" href="#id10" title="Permalink to this headline"></a></h3>
<ol class="arabic simple">
<li><p>Add your public SSH key (<code class="docutils literal notranslate"><span class="pre">~/.ssh/id_rsa.pub</span></code>) to the Google Cloud instance, like <a class="reference external" href="https://cloud.google.com/compute/docs/connect/add-ssh-keys?cloudshell=true#gcloud">here</a>. Easiest is with Cloud shell, upload your public key, and run <code class="docutils literal notranslate"><span class="pre">gcloud</span> <span class="pre">compute</span> <span class="pre">os-login</span> <span class="pre">ssh-keys</span> <span class="pre">add</span>&#160;&#160;&#160; <span class="pre">--key-file=id_rsa.pub</span></code></p></li>
<li><p>Turn the <a class="reference external" href="https://console.cloud.google.com/net-security/firewall-manager/firewall-policies/list">firewall</a> setting (e.g. <code class="docutils literal notranslate"><span class="pre">hpc-small-net-fw-allow-iap-ingress</span></code>) to allow <code class="docutils literal notranslate"><span class="pre">0.0.0.0/0</span></code> as IP ranges for <code class="docutils literal notranslate"><span class="pre">tcp:22</span></code>.</p></li>
<li><p>Promote the login node’s IP address to a static one: <a class="reference external" href="https://cloud.google.com/compute/docs/ip-addresses/reserve-static-external-ip-address#promote_ephemeral_ip">here</a></p></li>
<li><p>Copy that IP and your username.</p></li>
<li><p>On your own computer, add a SSH config file, store it as <code class="docutils literal notranslate"><span class="pre">~/.ssh/config</span></code> (no extension) with the ip and user filled in:</p></li>
</ol>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span>Host gcslurm
	HostName &lt;fill-in-the-External-IP-of-VM-instance&gt;
	User &lt;fill-in-your-Google-Cloud-user&gt;
	Port 22
	IdentityFile ~/.ssh/id_rsa
</pre></div>
</div>
<details>
  <summary>Details</summary>
<p>We need to setup our library with SSH access between OMERO and Slurm, but this is not built-in to these Virtual Machines yet.
We will forward our local SSH to our OMERO (in this tutorial), so we just need to setup SSH access to the Google Cloud VMs.</p>
<p>This sounds easier than it actually is.</p>
<p>Follow the steps at <a class="reference external" href="https://cloud.google.com/compute/docs/connect/add-ssh-keys?cloudshell=true#gcloud">here</a>:</p>
<ol class="arabic simple" start="0">
<li><p>Note that this tutorial by default seems to use the “OS Login” method, using the mail account you signed up with.</p></li>
<li><p>Open a Cloud Shell</p></li>
<li><p>Upload your public key to this Cloud Shell (with the <code class="docutils literal notranslate"><span class="pre">...</span></code> button).</p></li>
<li><p>Run the <code class="docutils literal notranslate"><span class="pre">gcloud</span> <span class="pre">compute</span> <span class="pre">os-login</span> <span class="pre">ssh-keys</span> <span class="pre">add</span>&#160;&#160;&#160; <span class="pre">--key-file=id_rsa.pub</span></code> command they show, pointing at your newly uploaded public key. Leave out the optional <code class="docutils literal notranslate"><span class="pre">project</span></code> and <code class="docutils literal notranslate"><span class="pre">expire_time</span></code>.</p></li>
</ol>
<p>Then, we have to ensure that the firewall accepts requests from outside Google Cloud, if it doesn’t already.</p>
<p>Go to the <a class="reference external" href="https://console.cloud.google.com/net-security/firewall-manager/firewall-policies/list">firewall</a> settings and edit the tcp:22 (e.g. <code class="docutils literal notranslate"><span class="pre">hpc-small-net-fw-allow-iap-ingress</span></code>) and add the <code class="docutils literal notranslate"><span class="pre">0.0.0.0/0</span></code> ip ranges.</p>
<p>Now we are ready:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">ssh</span> <span class="pre">-i</span> <span class="pre">~/.ssh/id_rsa</span> <span class="pre">&lt;fill-in-your-Google-Cloud-user&gt;&#64;&lt;fill-in-the-External-IP-of-VM-instance&gt;</span></code></p></li>
</ul>
<p>E.g. my Google Cloud user became <code class="docutils literal notranslate"><span class="pre">t_t_luik_amsterdamumc_nl</span></code>, related to the email I signed up with.
The External IP was on the <a class="reference external" href="https://console.cloud.google.com/compute/instances">VM instances</a> page for the login node <code class="docutils literal notranslate"><span class="pre">hpcsmall-login-2aoamjs0-001</span></code>.</p>
<p>Now to make this connection easy, we need 2 steps:</p>
<ol class="arabic simple">
<li><p>Fix this external IP address, so that it will always be the same</p></li>
<li><p>Fix a SSH config file for this SSH connection</p></li>
</ol>
<p>For 1, we got to <a class="reference external" href="https://cloud.google.com/compute/docs/ip-addresses/reserve-static-external-ip-address#promote_ephemeral_ip">here</a> and follow the Console steps to promote the IP address to a static IP address. Now back in the <code class="docutils literal notranslate"><span class="pre">All</span></code> screen, your newly named Static IP address should show up. Copy that IP (it should be the same IP as before, but now it will not change when you restart the system)</p>
<p>For 2, On your own computer, add a SSH config file, store it as <code class="docutils literal notranslate"><span class="pre">~/.ssh/config</span></code> (no extension) with the ip and user filled in:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span>Host gcslurm
	HostName &lt;fill-in-the-External-IP-of-VM-instance&gt;
	User &lt;fill-in-your-Google-Cloud-user&gt;
	Port 22
	IdentityFile ~/.ssh/id_rsa
</pre></div>
</div>
<p>Now you should be able to login with a simple: <code class="docutils literal notranslate"><span class="pre">ssh</span> <span class="pre">gcslurm</span></code>.</p>
<p>Congratulations!</p>
</details>
</div>
</div>
<div class="section" id="id11">
<h2>3. Test Slurm<a class="headerlink" href="#id11" title="Permalink to this headline"></a></h2>
<div class="section" id="id12">
<h3>TL;DR:<a class="headerlink" href="#id12" title="Permalink to this headline"></a></h3>
<ol class="arabic simple">
<li><p>SSH into the login node: <code class="docutils literal notranslate"><span class="pre">ssh</span> <span class="pre">gcslurm</span></code></p></li>
<li><p>Start some filler jobs: <code class="docutils literal notranslate"><span class="pre">sbatch</span> <span class="pre">--wrap=&quot;sleep</span> <span class="pre">5</span> <span class="pre">&amp;&amp;</span> <span class="pre">hostname&quot;</span> <span class="pre">&amp;&amp;</span>&#160; <span class="pre">sbatch</span> <span class="pre">--wrap=&quot;sleep</span> <span class="pre">5</span> <span class="pre">&amp;&amp;</span> <span class="pre">hostname&quot;</span> <span class="pre">&amp;&amp;</span>&#160; <span class="pre">sbatch</span> <span class="pre">--wrap=&quot;sleep</span> <span class="pre">5</span> <span class="pre">&amp;&amp;</span> <span class="pre">hostname&quot;</span> <span class="pre">&amp;&amp;</span>&#160; <span class="pre">sbatch</span> <span class="pre">--wrap=&quot;sleep</span> <span class="pre">5</span> <span class="pre">&amp;&amp;</span> <span class="pre">hostname&quot;</span></code></p></li>
<li><p>Check the progress: <code class="docutils literal notranslate"><span class="pre">squeue</span></code></p></li>
<li><p>Check some output when its done, e.g. job 1: <code class="docutils literal notranslate"><span class="pre">cat</span> <span class="pre">slurm-1.out</span></code></p></li>
</ol>
<details>
  <summary>Details</summary>
<p>Now connect via SSH to Google Cloud Slurm and let’s see if Slurm works:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>t_t_luik_amsterdamumc_nl@hpcsmall-login-2aoamjs0-001 ~<span class="o">]</span>$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST<span class="o">(</span>REASON<span class="o">)</span>
<span class="o">[</span>t_t_luik_amsterdamumc_nl@hpcsmall-login-2aoamjs0-001 ~<span class="o">]</span>$ sbatch --wrap<span class="o">=</span><span class="s2">&quot;sleep 5 &amp;&amp; hostname&quot;</span> <span class="o">&amp;&amp;</span>  sbatch --wrap<span class="o">=</span><span class="s2">&quot;sleep 5 &amp;&amp; hostname&quot;</span> <span class="o">&amp;&amp;</span>  sbatch --wrap<span class="o">=</span><span class="s2">&quot;sleep 5 &amp;&amp; hostname&quot;</span> <span class="o">&amp;&amp;</span>  sbatch --wrap<span class="o">=</span><span class="s2">&quot;sleep 5 &amp;&amp; hostname&quot;</span>
Submitted batch job <span class="m">4</span>
Submitted batch job <span class="m">5</span>
Submitted batch job <span class="m">6</span>
Submitted batch job <span class="m">7</span>
<span class="o">[</span>t_t_luik_amsterdamumc_nl@hpcsmall-login-2aoamjs0-001 ~<span class="o">]</span>$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST<span class="o">(</span>REASON<span class="o">)</span>
                 <span class="m">4</span>     debug     wrap t_t_luik CF       <span class="m">0</span>:03      <span class="m">1</span> hpcsmall-debug-ghpc-3
                 <span class="m">5</span>     debug     wrap t_t_luik PD       <span class="m">0</span>:00      <span class="m">1</span> <span class="o">(</span>Resources<span class="o">)</span>
                 <span class="m">6</span>     debug     wrap t_t_luik PD       <span class="m">0</span>:00      <span class="m">1</span> <span class="o">(</span>Priority<span class="o">)</span>
                 <span class="m">7</span>     debug     wrap t_t_luik PD       <span class="m">0</span>:00      <span class="m">1</span> <span class="o">(</span>Priority<span class="o">)</span>
</pre></div>
</div>
<p>I fired off 4 jobs that take some seconds, so they are still in the queue by the time I call the <code class="docutils literal notranslate"><span class="pre">squeue</span></code>. Note that the first one might take a while since Google Cloud has to fire up a new compute node for the first time.</p>
<p>The jobs wrote their stdout output in the current dir:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>t_t_luik_amsterdamumc_nl@hpcsmall-login-2aoamjs0-001 ~<span class="o">]</span>$ ls
slurm-4.out  slurm-5.out  slurm-6.out  slurm-7.out
<span class="o">[</span>t_t_luik_amsterdamumc_nl@hpcsmall-login-2aoamjs0-001 ~<span class="o">]</span>$ squeue
             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST<span class="o">(</span>REASON<span class="o">)</span>
<span class="o">[</span>t_t_luik_amsterdamumc_nl@hpcsmall-login-2aoamjs0-001 ~<span class="o">]</span>$ cat slurm-4.out
hpcsmall-debug-ghpc-3
<span class="o">[</span>t_t_luik_amsterdamumc_nl@hpcsmall-login-2aoamjs0-001 ~<span class="o">]</span>$ cat slurm-5.out
hpcsmall-debug-ghpc-3
</pre></div>
</div>
<p>All on the same node that was spun up, on-demand, by Google Cloud. You should be able to see it still alive in the <code class="docutils literal notranslate"><span class="pre">VM</span> <span class="pre">instances</span></code> <a class="reference external" href="https://console.cloud.google.com/compute/instances">tab</a> as well. It will be destroyed again if not used for a while, saving you costs.</p>
</details>
</div>
</div>
<div class="section" id="b-install-requirements-singularity-apptainer-and-7zip">
<h2>3b. Install requirements: Singularity / Apptainer and 7zip<a class="headerlink" href="#b-install-requirements-singularity-apptainer-and-7zip" title="Permalink to this headline"></a></h2>
<div class="section" id="id13">
<h3>TL;DR:<a class="headerlink" href="#id13" title="Permalink to this headline"></a></h3>
<ol class="arabic simple">
<li><p>Follow this <a class="reference external" href="https://cloud.google.com/architecture/deploying-containerized-workloads-slurm-cluster-compute-engine">guide</a> to install Singularity, but in step 5 please install in <code class="docutils literal notranslate"><span class="pre">/opt/apps</span></code> ! <code class="docutils literal notranslate"><span class="pre">/apps</span></code> is not actually shared with all nodes.</p></li>
<li><p>Execute the following to update <code class="docutils literal notranslate"><span class="pre">~/.bashrc</span></code>:</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">echo</span> <span class="s1">&#39;export PATH=/apps/singularity/3.8.7/bin:/usr/sbin:$</span><span class="si">{PATH}</span><span class="s1">&#39;</span> <span class="o">&gt;&gt;</span> <span class="o">~/.</span><span class="n">bashrc</span> <span class="o">&amp;&amp;</span> <span class="n">source</span> <span class="o">~/.</span><span class="n">bashrc</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>Install 7zip: <code class="docutils literal notranslate"><span class="pre">sudo</span> <span class="pre">yum</span> <span class="pre">install</span> <span class="pre">-y</span> <span class="pre">p7zip</span> <span class="pre">p7zip-plugins</span></code></p></li>
</ol>
<details>
<p>Now we want to run containers on our Slurm cluster using <code class="docutils literal notranslate"><span class="pre">singularity</span></code>, but this is not installed by default.</p>
<p>Luckily the folks at Google have a <a class="reference external" href="https://cloud.google.com/architecture/deploying-containerized-workloads-slurm-cluster-compute-engine">guide</a> for it, so let’s follow that one.</p>
<p>If the ssh connection to the login node doesn’t work from Google Cloud Shell, you can continue with the steps by using the SSH connection (<code class="docutils literal notranslate"><span class="pre">ssh</span> <span class="pre">gcslurm</span></code>) that we just built from your local commandline.</p>
<p>Use this URL for the singularity tar:</p>
<p><code class="docutils literal notranslate"><span class="pre">https://github.com/apptainer/singularity/releases/download/v3.8.7/singularity-3.8.7.tar.gz</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>wget https://github.com/apptainer/singularity/releases/download/v3.8.7/singularity-3.8.7.tar.gz <span class="o">&amp;&amp;</span> tar -xzf singularity-<span class="si">${</span><span class="nv">SINGULARITY_VERSION</span><span class="si">}</span>.tar.gz <span class="o">&amp;&amp;</span> <span class="nb">cd</span> singularity-<span class="si">${</span><span class="nv">SINGULARITY_VERSION</span><span class="si">}</span>
</pre></div>
</div>
<p>The module step did not work for me, because it is the wrong directory in the guide!</p>
<p>In step 5, we need to install to <code class="docutils literal notranslate"><span class="pre">/opt/apps</span></code> instead! This is very important because the compute nodes that have to execute the job need to have access to this software too, and this directory is the actual shared directory:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./mconfig --prefix<span class="o">=</span>/opt/apps/singularity/<span class="si">${</span><span class="nv">SINGULARITY_VERSION</span><span class="si">}</span> <span class="o">&amp;&amp;</span> <span class="se">\</span>
    make -C ./builddir <span class="o">&amp;&amp;</span> <span class="se">\</span>
    sudo make -C ./builddir install
</pre></div>
</div>
<p>Now <code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">avail</span></code> should list <code class="docutils literal notranslate"><span class="pre">singularity</span></code>.</p>
<p>So <code class="docutils literal notranslate"><span class="pre">module</span> <span class="pre">load</span> <span class="pre">singularity</span></code> and now <code class="docutils literal notranslate"><span class="pre">singularity</span> <span class="pre">--version</span></code> should give you <code class="docutils literal notranslate"><span class="pre">singularity</span> <span class="pre">version</span> <span class="pre">3.8.7</span></code>.</p>
</details>
<p>Now let’s connect OMERO to our Slurm!</p>
</div>
</div>
<div class="section" id="id14">
<h2>4. OMERO &amp; OMERO Slurm Client<a class="headerlink" href="#id14" title="Permalink to this headline"></a></h2>
<p>Ok, now we need a OMERO server and a correctly configured OMERO Slurm Client.</p>
<div class="section" id="id15">
<h3>TL;DR:<a class="headerlink" href="#id15" title="Permalink to this headline"></a></h3>
<ol class="arabic simple">
<li><p>Clone my example <code class="docutils literal notranslate"><span class="pre">docker-example-omero-grid-amc</span></code> locally: <code class="docutils literal notranslate"><span class="pre">git</span> <span class="pre">clone</span> <span class="pre">-b</span> <span class="pre">processors</span> <span class="pre">https://github.com/TorecLuik/docker-example-omero-grid-amc.git</span></code></p></li>
<li><p>Change the <code class="docutils literal notranslate"><span class="pre">worker-gpu/slurm-config.ini</span></code> file to point to <code class="docutils literal notranslate"><span class="pre">worker-gpu/slurm-config.gcslurm.ini</span></code> file (if it is not the same file already)</p></li>
<li><p>Fire up the OMERO containers: <code class="docutils literal notranslate"><span class="pre">docker-compose</span> <span class="pre">up</span> <span class="pre">-d</span> <span class="pre">--build</span></code></p></li>
<li><p>Go to OMERO.web (<code class="docutils literal notranslate"><span class="pre">localhost:4080</span></code>), login <code class="docutils literal notranslate"><span class="pre">root</span></code> pw <code class="docutils literal notranslate"><span class="pre">omero</span></code></p></li>
<li><p>Upload some images (to <code class="docutils literal notranslate"><span class="pre">localhost</span></code>) with OMERO.Insight (not included).</p></li>
<li><p>In web, run the <code class="docutils literal notranslate"><span class="pre">slurm/init_environment</span></code> script</p></li>
</ol>
<details>
  <summary>Details</summary>
<p>======= OMERO in Docker =======</p>
<p>You can use your own OMERO setup, but for this tutorial I will refer to a dockerized OMERO that I am working with: <a class="reference external" href="https://github.com/TorecLuik/docker-example-omero-grid-amc/tree/processors">get it here</a>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git clone -b processors https://github.com/TorecLuik/docker-example-omero-grid-amc.git
</pre></div>
</div>
<p>Change the <code class="docutils literal notranslate"><span class="pre">worker-gpu/slurm-config.ini</span></code> file to be the <code class="docutils literal notranslate"><span class="pre">worker-gpu/slurm-config.gcslurm.ini</span></code> file (if it is not the same file already).</p>
<p>What we did was point to <code class="docutils literal notranslate"><span class="pre">gcslurm</span></code> profile (or rename your SSH profile to <code class="docutils literal notranslate"><span class="pre">slurm</span></code>)</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[SSH]</span>
<span class="c1"># -------------------------------------</span>
<span class="c1"># SSH settings</span>
<span class="c1"># -------------------------------------</span>
<span class="c1"># The alias for the SLURM SSH connection</span>
<span class="na">host</span><span class="o">=</span><span class="s">gcslurm</span>
</pre></div>
</div>
<p>And we also set all directories to be relative to the home dir, and we reduced CellPose CPU drastically to fit into the small Slurm cluster we made in Google Cloud.</p>
<p>This way, it will use the right SSH setting to connect with our Google Cloud Slurm.</p>
<p>Let’s (build it and) fire it up:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker-compose up -d --build
</pre></div>
</div>
<p>======= OMERO web =======</p>
<p>Once they are running, you should be able to access web at <code class="docutils literal notranslate"><span class="pre">localhost:4080</span></code>. Login with user <code class="docutils literal notranslate"><span class="pre">root</span></code> / pw <code class="docutils literal notranslate"><span class="pre">omero</span></code>.</p>
<p>Import some example data with OMERO Insight (connect with <code class="docutils literal notranslate"><span class="pre">localhost</span></code>).</p>
<p>======= Connect to Slurm =======</p>
<p>This container’s processor node (<code class="docutils literal notranslate"><span class="pre">worker-5</span></code>) has already installed our <code class="docutils literal notranslate"><span class="pre">omero-slurm-client</span></code> library.</p>
<p>======= Add ssh config to OMERO Processor =======</p>
<p>Ok, so SSH works fine from your machine, but we need the OMERO processing server <code class="docutils literal notranslate"><span class="pre">worker-5</span></code> to be able to do it too.</p>
<p>By some smart tricks, we have mounted our <code class="docutils literal notranslate"><span class="pre">~/.ssh</span></code> folder to the worker container, so it knows and can use our SSH settings and config.</p>
<p>Ok, so now we can connect from within the worker-5 to our Slurm cluster. We can try it out:</p>
<div class="highlight-powershell notranslate"><div class="highlight"><pre><span></span><span class="p">...\</span><span class="n">docker-example-omero-grid</span><span class="p">&gt;</span> <span class="n">docker-compose</span> <span class="n">exec</span> <span class="n">omeroworker</span><span class="p">-</span><span class="n">5</span> <span class="p">/</span><span class="n">bin</span><span class="p">/</span><span class="n">bash</span>
<span class="n">bash</span><span class="p">-</span><span class="n">4</span><span class="p">.</span><span class="n">2</span><span class="p">$</span> <span class="n">ssh</span> <span class="n">gcslurm</span>

<span class="p">&lt;</span><span class="n">pretty-slurm-art</span><span class="p">&gt;</span>

<span class="p">[</span><span class="n">t_t_luik_amsterdamumc_nl</span><span class="nv">@hpcsmall</span><span class="n">-login</span><span class="p">-</span><span class="n">2aoamjs0</span><span class="p">-</span><span class="n">001</span> <span class="p">~]$</span> <span class="n">squeue</span>
             <span class="n">JOBID</span> <span class="n">PARTITION</span>     <span class="n">NAME</span>     <span class="n">USER</span> <span class="n">ST</span>       <span class="n">TIME</span>  <span class="n">NODES</span> <span class="n">NODELIST</span><span class="p">(</span><span class="n">REASON</span><span class="p">)</span>
</pre></div>
</div>
<p>======= Init environment =======</p>
<p>Now we go to OMERO web and run the <code class="docutils literal notranslate"><span class="pre">slurm/init_environment</span></code> script to apply this config and setup our Slurm. We will use the default location, no need to fill in anything, just run the script.</p>
<p><img alt="Slurm Init Busy" src="images/webclient_init_env.PNG" /></p>
<p><img alt="Slurm Init Done" src="images/webclient_init_env_done.PNG" /></p>
</details>
<p>Note, this will take a while, since it is downloading workflow docker images and building (singularity) containers from them.</p>
<p>Congratulations! We have setup workflows CellPose <code class="docutils literal notranslate"><span class="pre">v1.2.7</span></code>, Cellprofiler Spot <code class="docutils literal notranslate"><span class="pre">v1.0.1</span></code> and CellExpansion <code class="docutils literal notranslate"><span class="pre">v1.0.1</span></code>. And there are no data files yet.</p>
<p>Let’s go run some segmentation workflow then!</p>
</div>
</div>
<div class="section" id="id16">
<h2>5. Workflows!<a class="headerlink" href="#id16" title="Permalink to this headline"></a></h2>
<div class="section" id="id17">
<h3>TL;DR:<a class="headerlink" href="#id17" title="Permalink to this headline"></a></h3>
<ol class="arabic simple">
<li><p>In web, select your images and run script <code class="docutils literal notranslate"><span class="pre">slurm/SLURM</span> <span class="pre">Run</span> <span class="pre">Workflow</span></code></p>
<ul class="simple">
<li><p>Tick off <code class="docutils literal notranslate"><span class="pre">E-mail</span></code> box (not implemented in this Slurm docker setup)</p></li>
<li><p>For importing results, change <code class="docutils literal notranslate"><span class="pre">3a)</span> <span class="pre">Import</span> <span class="pre">into</span> <span class="pre">NEW</span> <span class="pre">Dataset</span></code> to <code class="docutils literal notranslate"><span class="pre">CellPose_Masks</span></code></p></li>
<li><p>For importing results, change <code class="docutils literal notranslate"><span class="pre">3b)</span> <span class="pre">Rename</span> <span class="pre">the</span> <span class="pre">imported</span> <span class="pre">images</span></code> to <code class="docutils literal notranslate"><span class="pre">{original_file}_cpmask.{ext}</span></code></p></li>
<li><p>Select <code class="docutils literal notranslate"><span class="pre">cellpose</span></code>, but tick off <code class="docutils literal notranslate"><span class="pre">use_gpu</span></code> off (sadly not implemented in this docker setup)</p></li>
<li><p>Click <code class="docutils literal notranslate"><span class="pre">Run</span> <span class="pre">Script</span></code></p></li>
</ul>
</li>
<li><p>Now go get a coffee or something, it should take a lot of minutes (about 12m:30s for 4 256x256 images for me!) and then say (a.o.): <code class="docutils literal notranslate"><span class="pre">COMPLETE</span></code></p>
<ul class="simple">
<li><p>Or it <code class="docutils literal notranslate"><span class="pre">FAILED</span></code>, in which case you should check all the details anyway and get your hands dirty with debugging! Or try less and smaller images.</p></li>
</ul>
</li>
<li><p>Refresh your Explore window, there should be a new dataset <code class="docutils literal notranslate"><span class="pre">CellPose_Masks</span></code> with a mask for every input image.</p></li>
</ol>
<details>
  <summary>Details</summary>
<p>So, I hope you added some data already; if not, import some images now.</p>
<p>Let’s run <code class="docutils literal notranslate"><span class="pre">slurm/SLURM</span> <span class="pre">Run</span> <span class="pre">Workflow</span></code>:</p>
<p><img alt="Slurm Run Workflow" src="images/webclient_run_workflow.PNG?raw=true" /></p>
<p>You can see that this script recognized that we downloaded 3 workflows, and what their parameters are. For more information on this magic, follow the other tutorials.</p>
<p>Let’s select <code class="docutils literal notranslate"><span class="pre">cellpose</span></code> and click <code class="docutils literal notranslate"><span class="pre">use</span> <span class="pre">gpu</span></code> off (sadly). Tune the other parameters as you like for your images. Also, for output let’s select <code class="docutils literal notranslate"><span class="pre">Import</span> <span class="pre">into</span> <span class="pre">NEW</span> <span class="pre">Dataset</span></code> by filling in a dataset name: cellpose_images. Click <code class="docutils literal notranslate"><span class="pre">Run</span> <span class="pre">Script</span></code>.</p>
<p><img alt="Slurm Run Cellpose" src="images/webclient_run_cellpose.PNG?raw=true" /></p>
<p>This will take ages because we did not invest in good compute on the Slurm cluster. It took 12m:30s for 4 small images for me.</p>
<p>You can check the progress with the <code class="docutils literal notranslate"><span class="pre">Slurm</span> <span class="pre">Get</span> <span class="pre">Update</span></code> script.</p>
</details>
<p>That should take you through connecting OMERO with a Google Cloud Slurm setup!</p>
</div>
</div>
</div>
<hr class="docutils" />
<div class="section" id="microsoft-azure-slurm-tutorial">
<h1>Microsoft Azure Slurm tutorial<a class="headerlink" href="#microsoft-azure-slurm-tutorial" title="Permalink to this headline"></a></h1>
<div class="section" id="id18">
<h2>Introduction<a class="headerlink" href="#id18" title="Permalink to this headline"></a></h2>
<p>This library is meant to be used with some external HPC cluster using Slurm, to offload your (OMERO) compute to servers suited for it.</p>
<p>However, if you don’t have ready access (yet) to such a cluster, you might want to spin some test environment up in the Cloud and connect your (local) OMERO to it.
This is what we will cover in this tutorial, specifically <strong>Microsoft Azure</strong>.</p>
</div>
<div class="section" id="id19">
<h2>0. Requirements<a class="headerlink" href="#id19" title="Permalink to this headline"></a></h2>
<p>To follow this tutorial, you need:</p>
<ul class="simple">
<li><p>OMERO Insight</p></li>
<li><p>An Azure account (and credits)</p></li>
</ul>
<p>I try to provide a tl;dr when I can, otherwise I go step by step.</p>
</div>
<div class="section" id="setup-microsoft-azure-for-slurm">
<h2>1. Setup Microsoft Azure for Slurm<a class="headerlink" href="#setup-microsoft-azure-for-slurm" title="Permalink to this headline"></a></h2>
<div class="section" id="id20">
<h3>TL;DR:<a class="headerlink" href="#id20" title="Permalink to this headline"></a></h3>
<ol class="arabic simple">
<li><p>Make a new Azure account if you don’t have one. Hopefully you get/have some free credits.</p></li>
<li><p>Create an new App “BIOMERO” via “App registrations”</p>
<ul class="simple">
<li><p>Copy Application ID</p></li>
<li><p>Copy Application Secret</p></li>
</ul>
</li>
<li><p>Assign roles to App “BIOMERO”:</p>
<ul class="simple">
<li><p>“Azure Container Storage Operator” role on the Subscription (or probably on the Resource Group works too)</p></li>
<li><p>“Virtual Machine Contributor” role on the Resource Group (“biomero-public”)</p></li>
<li><p>“Network Contributor” role on the Resource Group (“biomero-public”)</p></li>
</ul>
</li>
<li><p>Create storageaccount “biomerostorage” in the “biomero-public” Resource Group</p></li>
<li><p>Mainly: Follow this video <a class="reference external" href="https://www.youtube.com/watch?v=qIm6PAFVsmU">tutorial</a> from Microsoft Azure</p>
<ul class="simple">
<li><p>However, note that I actually have trouble with their specific version of Slurm in CycleCloud and the default version works fine. Checkout the <code class="docutils literal notranslate"><span class="pre">details</span></code> below for more details on this part.</p></li>
</ul>
</li>
<li><p>Probably use something cheaper than 4x the expensive <code class="docutils literal notranslate"><span class="pre">Standard_ND96amsr_A100_v4</span></code> instances, unless you are really rich!</p>
<ul class="simple">
<li><p>Note: Use <code class="docutils literal notranslate"><span class="pre">Ds</span></code> not <code class="docutils literal notranslate"><span class="pre">Das</span></code> or <code class="docutils literal notranslate"><span class="pre">Des</span></code> VM types, if you run into <code class="docutils literal notranslate"><span class="pre">security</span> <span class="pre">type</span> <span class="pre">&lt;null&gt;</span></code> errors in deployment.</p></li>
</ul>
</li>
<li><p>We need a Slurm accounting database for BIOMERO! See <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">-</span> <span class="pre">Addendum</span></code> chapter below for setting one up, if you don’t have a database.</p></li>
<li><p>Add a public key to your Azure CycleCloud profile. Probably use the <code class="docutils literal notranslate"><span class="pre">hpc-slurm-cluster_key</span></code> that you can find in your Resource Group.</p></li>
<li><p>Now you should be able to login to the Slurm scheduler with something like <code class="docutils literal notranslate"><span class="pre">ssh</span> <span class="pre">-i</span> <span class="pre">C:\&lt;path-to-my&gt;\hpc-slurm-cluster_key.pem</span> <span class="pre">azureadmin&#64;&lt;scheduler-vm-public-ip&gt;</span></code></p></li>
<li><p>Change the cloud-init to install Singularity and 7zip on your nodes.</p></li>
</ol>
<details>
  <summary>Details</summary>
<p>So, we follow this <a class="reference external" href="https://www.youtube.com/watch?v=qIm6PAFVsmU">tutorial</a> and end up with a <code class="docutils literal notranslate"><span class="pre">hpc-slurm-cluster</span></code> (that’s what I named the VM) VM on Microsoft Azure. It also downloaded the SSH private key for us (<code class="docutils literal notranslate"><span class="pre">hpc-slurm-cluster_key.pem</span></code>).</p>
</div>
<hr class="docutils" />
<div class="section" id="suggested-alternative-use-a-basic-slurm-cluster">
<h3>Suggested alternative: use a basic Slurm cluster<a class="headerlink" href="#suggested-alternative-use-a-basic-slurm-cluster" title="Permalink to this headline"></a></h3>
<p>CycleCloud already comes with a basic Slurm setup, that is more up-to-date than this specific GPU powered version.
Especially if you will not use GPU anyway (because $$).</p>
<p>So, given you followed the movie to get a CycleCloud VM up and running, let’s setup a basic Slurm cluster instead.</p>
<p>Let’s start that up:</p>
<ul class="simple">
<li><p>Click <code class="docutils literal notranslate"><span class="pre">+</span></code> / <code class="docutils literal notranslate"><span class="pre">Add</span></code> for a new cluster and select <code class="docutils literal notranslate"><span class="pre">Slurm</span></code> (instead of <code class="docutils literal notranslate"><span class="pre">cc-slurm-ngc</span></code>)</p></li>
<li><p>We provide a new name <code class="docutils literal notranslate"><span class="pre">biomero-cluster-basic</span></code></p></li>
<li><p>We change all the VM types:</p>
<ul>
<li><p>Scheduler: <code class="docutils literal notranslate"><span class="pre">Standard_DC4s_v3</span></code></p></li>
<li><p>HPC, HTC and Dyn: <code class="docutils literal notranslate"><span class="pre">Standard_DC2s_v3</span></code></p></li>
<li><p>Login node we will not use so doesn’t matter (<code class="docutils literal notranslate"><span class="pre">Num</span> <span class="pre">Login</span> <span class="pre">Nodes</span></code> stays <code class="docutils literal notranslate"><span class="pre">0</span></code>)</p></li>
</ul>
</li>
<li><p>We change the scaling amount to only 4 cores each (instead of 100), and MaxVMs to 2.</p></li>
<li><p>Change the network to the default biomero network</p></li>
<li><p>Next, keep all the <code class="docutils literal notranslate"><span class="pre">Network</span> <span class="pre">Attached</span> <span class="pre">Storage</span></code> settings</p></li>
<li><p>Next, <code class="docutils literal notranslate"><span class="pre">Advanced</span> <span class="pre">Settings</span></code></p>
<ul>
<li><p>Here, we need to do 2 major things:</p></li>
<li><p>First, add the Slurm accounting database. See <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">-</span> <span class="pre">Addendum</span></code> chapter below for setting that up.</p></li>
<li><p>Second, select appropriate VM images that will work for our software (mainly <code class="docutils literal notranslate"><span class="pre">singularity</span></code> for containers): <code class="docutils literal notranslate"><span class="pre">Ubuntu</span> <span class="pre">22.04</span> <span class="pre">LTS</span></code> worked for us.</p></li>
</ul>
</li>
<li><p>Next, keep all the security settings</p></li>
<li><p>Finally, let’s <a class="reference external" href="https://learn.microsoft.com/en-us/answers/questions/404297/installing-singularity-on-a-slurm-cyclecloud-clust">change Cloud init</a> for <em>all</em> nodes, to install singularity and 7zip:</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">#cloud-config  </span>
<span class="n">package_upgrade</span><span class="p">:</span> <span class="n">true</span>
<span class="n">packages</span><span class="p">:</span>  
  <span class="o">-</span> <span class="n">htop</span>
  <span class="o">-</span> <span class="n">wget</span>
  <span class="o">-</span> <span class="n">p7zip</span><span class="o">-</span><span class="n">full</span>
  <span class="o">-</span> <span class="n">software</span><span class="o">-</span><span class="n">properties</span><span class="o">-</span><span class="n">common</span>

<span class="n">runcmd</span><span class="p">:</span>  
  <span class="o">-</span> <span class="s1">&#39;sudo add-apt-repository -y ppa:apptainer/ppa&#39;</span>
  <span class="o">-</span> <span class="s1">&#39;sudo apt update&#39;</span>
  <span class="o">-</span> <span class="s1">&#39;sudo apt install -y apptainer&#39;</span>
</pre></div>
</div>
<p>Apptainer is singularity and will provide the singularity (alias) command.</p>
</details>
</div>
</div>
<div class="section" id="addendum-setting-up-slurm-accounting-db">
<h2>1 - Addendum - Setting up Slurm Accounting DB<a class="headerlink" href="#addendum-setting-up-slurm-accounting-db" title="Permalink to this headline"></a></h2>
<ol class="arabic simple" start="0">
<li><p>Expected price: 16.16 euro / month</p></li>
<li><p>We follow <a class="reference external" href="https://techcommunity.microsoft.com/t5/azure-high-performance-computing/setting-up-slurm-job-accounting-with-azure-cyclecloud-and-azure/ba-p/4083685">this recent blog</a></p>
<ul class="simple">
<li><p>Note that <code class="docutils literal notranslate"><span class="pre">Western</span> <span class="pre">Europe</span></code> has deployment issues on Azure for these DB. See <code class="docutils literal notranslate"><span class="pre">Details</span></code> for more details.</p></li>
</ul>
</li>
</ol>
<details>
  <summary>Details</summary>
<div class="section" id="a-create-extra-subnet-on-your-virtual-network">
<h3>A. Create extra subnet on your virtual network<a class="headerlink" href="#a-create-extra-subnet-on-your-virtual-network" title="Permalink to this headline"></a></h3>
<ul class="simple">
<li><p>Go to your <code class="docutils literal notranslate"><span class="pre">hpc-slurm-cluster-vnet</span></code></p></li>
<li><p>Go to <code class="docutils literal notranslate"><span class="pre">Subnets</span></code> Settings</p></li>
<li><p>Create a new subnet with <code class="docutils literal notranslate"><span class="pre">+</span> <span class="pre">Subnet</span></code>, named <code class="docutils literal notranslate"><span class="pre">mysql</span></code> (and default settings)</p></li>
</ul>
</div>
<div class="section" id="b-azure-database">
<h3>B. Azure database<a class="headerlink" href="#b-azure-database" title="Permalink to this headline"></a></h3>
<p>We create a <a class="reference external" href="https://portal.azure.com/#create/Microsoft.MySQLFlexibleServer">MySQL Flexible Server</a></p>
<ul class="simple">
<li><p>Server name <code class="docutils literal notranslate"><span class="pre">slurm-accounting-database</span></code> (or whatever is available)</p></li>
<li><p>Region <code class="docutils literal notranslate"><span class="pre">Western</span> <span class="pre">Europe</span></code> (same as your server/VNET).</p>
<ul>
<li><p>See Notes below about issues (and solutions) in <code class="docutils literal notranslate"><span class="pre">Western</span> <span class="pre">Europe</span></code> at the time of writing.</p></li>
</ul>
</li>
<li><p>MySQL version <code class="docutils literal notranslate"><span class="pre">8.0</span></code></p></li>
<li><p>Workload type <code class="docutils literal notranslate"><span class="pre">For</span> <span class="pre">development</span></code> (unless you are being serious)</p></li>
<li><p>Authentication method <code class="docutils literal notranslate"><span class="pre">MySQL</span> <span class="pre">authentication</span> <span class="pre">only</span></code></p></li>
<li><p>User credentials that you like, I used <code class="docutils literal notranslate"><span class="pre">omero</span></code> user.</p></li>
<li><p>Next: Networking</p>
<ul>
<li><p>Connectivity method: <code class="docutils literal notranslate"><span class="pre">Private</span> <span class="pre">access</span> <span class="pre">(VNet</span> <span class="pre">Integration)</span></code></p></li>
<li><p>Virtual network: select your existing <code class="docutils literal notranslate"><span class="pre">hpc-slurm-cluster-vnet</span></code> net</p></li>
<li><p>Subnet: select your new subnet <code class="docutils literal notranslate"><span class="pre">hpc-slurm-cluster-vnet/mysql</span></code></p></li>
<li><p>Private DNS, let it create or use existing one.</p></li>
</ul>
</li>
<li><p>Next: deploy it!</p></li>
</ul>
<p>Next, let’s change some Server parameters according to the blog.
I think this is optional though.</p>
<ol class="arabic simple">
<li><p>Go to your <code class="docutils literal notranslate"><span class="pre">slurm-accounting-database</span></code> in the Azure portal</p></li>
<li><p>Open on left-hand side <code class="docutils literal notranslate"><span class="pre">Server</span> <span class="pre">parameters</span></code></p></li>
<li><p>Click on <code class="docutils literal notranslate"><span class="pre">All</span></code></p></li>
<li><p>Filter on <code class="docutils literal notranslate"><span class="pre">innodb_lock_wait_timeout</span></code></p></li>
<li><p>Change value to <code class="docutils literal notranslate"><span class="pre">900</span></code></p></li>
<li><p>Save changes</p></li>
</ol>
<p>Note! Availability issue in <code class="docutils literal notranslate"><span class="pre">Western</span> <span class="pre">Europe</span></code> in march 2024:</p>
<p>We had an issue deploying the database in <code class="docutils literal notranslate"><span class="pre">Western</span> <span class="pre">Europe</span></code>, apparently it is full there. So we deployed the database in <code class="docutils literal notranslate"><span class="pre">UK</span> <span class="pre">South</span></code> instead. If you have different regions, you need to connect the VNETs of both regions though, through something called <code class="docutils literal notranslate"><span class="pre">peering</span></code>!</p>
<p>For this to work, make sure the IPs of the subnets do not overlap, see <code class="docutils literal notranslate"><span class="pre">A</span></code> where we made an extra subnet with different IP.</p>
<ul class="simple">
<li><p>We made some extra <code class="docutils literal notranslate"><span class="pre">biomero-public-vnet</span></code> with a <code class="docutils literal notranslate"><span class="pre">mysql</span></code> subnet on the <code class="docutils literal notranslate"><span class="pre">10.1.0.0/24</span></code> range. Make sure it is also <code class="docutils literal notranslate"><span class="pre">Delegated</span> <span class="pre">to</span></code> <code class="docutils literal notranslate"><span class="pre">Microsoft.DBforMySQL/flexibleServers</span></code>.</p></li>
<li><p>Remove the <code class="docutils literal notranslate"><span class="pre">default</span></code> subnet</p></li>
<li><p>Remove the <code class="docutils literal notranslate"><span class="pre">10.0.0.0/24</span></code> address space (as we will connect the other vnet here)</p></li>
<li><p>Then go to the <code class="docutils literal notranslate"><span class="pre">biomero-public-vnet</span></code>, <code class="docutils literal notranslate"><span class="pre">Peerings</span></code> and <code class="docutils literal notranslate"><span class="pre">+</span> <span class="pre">Add</span></code> a new peering.</p></li>
<li><p>First named <code class="docutils literal notranslate"><span class="pre">hpc</span></code>, default settings</p></li>
<li><p>Remote named <code class="docutils literal notranslate"><span class="pre">acct</span></code>, connecting to the <code class="docutils literal notranslate"><span class="pre">hpc-slurm-cluster-vnet</span></code>.</p></li>
</ul>
</div>
<div class="section" id="c-slurm-accounting-settings">
<h3>C. Slurm Accounting settings<a class="headerlink" href="#c-slurm-accounting-settings" title="Permalink to this headline"></a></h3>
<p>Ok, now back in CycleCloud, we will set Slurm Accounting:</p>
<ol class="arabic simple">
<li><p>Edit cluster</p></li>
<li><p>Advanced Settings</p></li>
<li><p>Check the <code class="docutils literal notranslate"><span class="pre">Configure</span> <span class="pre">Slurm</span> <span class="pre">job</span> <span class="pre">accounting</span></code> box</p></li>
</ol>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Slurm</span> <span class="pre">DBD</span> <span class="pre">URL</span></code> will be
your chosen Server name (check the Azure portal). For me it was <code class="docutils literal notranslate"><span class="pre">slurm-accounting-database.mysql.database.azure.com</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Slurm</span> <span class="pre">DBD</span> <span class="pre">User</span></code> and <code class="docutils literal notranslate"><span class="pre">...</span> <span class="pre">Password</span></code> are what entered in deployment for the DB.</p></li>
<li><p>SSL Certificate URL is <code class="docutils literal notranslate"><span class="pre">https://dl.cacerts.digicert.com/DigiCertGlobalRootCA.crt.pem</span></code></p></li>
</ul>
<ol class="arabic simple" start="4">
<li><p>(Re)start your Slurm cluster.</p></li>
<li><p>Test out if the <code class="docutils literal notranslate"><span class="pre">sacct</span></code> command works!</p></li>
</ol>
</details>
</div>
</div>
<div class="section" id="id21">
<h2>2. Test Slurm<a class="headerlink" href="#id21" title="Permalink to this headline"></a></h2>
<ol class="arabic simple">
<li><p>SSH into the login node: <code class="docutils literal notranslate"><span class="pre">ssh</span> <span class="pre">gcslurm</span></code></p></li>
<li><p>Start some filler jobs: <code class="docutils literal notranslate"><span class="pre">sbatch</span> <span class="pre">--wrap=&quot;sleep</span> <span class="pre">5</span> <span class="pre">&amp;&amp;</span> <span class="pre">hostname&quot;</span> <span class="pre">&amp;&amp;</span>&#160; <span class="pre">sbatch</span> <span class="pre">--wrap=&quot;sleep</span> <span class="pre">5</span> <span class="pre">&amp;&amp;</span> <span class="pre">hostname&quot;</span> <span class="pre">&amp;&amp;</span>&#160; <span class="pre">sbatch</span> <span class="pre">--wrap=&quot;sleep</span> <span class="pre">5</span> <span class="pre">&amp;&amp;</span> <span class="pre">hostname&quot;</span> <span class="pre">&amp;&amp;</span>&#160; <span class="pre">sbatch</span> <span class="pre">--wrap=&quot;sleep</span> <span class="pre">5</span> <span class="pre">&amp;&amp;</span> <span class="pre">hostname&quot;</span></code></p></li>
<li><p>Check the progress: <code class="docutils literal notranslate"><span class="pre">squeue</span></code> (perhaps also check Azure CycleCloud to see your HPC VMs spinning up, takes a few min)</p></li>
<li><p>Check some output when its done, e.g. job 1: <code class="docutils literal notranslate"><span class="pre">cat</span> <span class="pre">slurm-1.out</span></code></p></li>
</ol>
</div>
<div class="section" id="test-singularity-on-slurm">
<h2>3. Test Singularity on Slurm<a class="headerlink" href="#test-singularity-on-slurm" title="Permalink to this headline"></a></h2>
<p>For example, run:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>sbatch -n 1 --wrap &quot;hostname &gt; lolcow.log &amp;&amp; singularity run docker://godlovedc/lolcow &gt;&gt; lolcow.log&quot;
</pre></div>
</div>
<p>This should say “Submitted batch job 1”
Then let’s tail the logfile:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>tail -f lolcow.log
</pre></div>
</div>
<p>First we see the slurm node that is computing, and later we will see the funny cow.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="o">[</span>slurm@slurmctld data<span class="o">]</span>$ tail -f lolcow.log
c1
 _______________________________________
/ Must I hold a candle to my shames?    <span class="se">\</span>
<span class="p">|</span>                                       <span class="p">|</span>
<span class="p">|</span> -- William Shakespeare, <span class="s2">&quot;The Merchant |</span>
<span class="s2">\ of Venice&quot;</span>                            /
 ---------------------------------------
        <span class="se">\ </span>  ^__^
         <span class="se">\ </span> <span class="o">(</span>oo<span class="o">)</span><span class="se">\_</span>______
            <span class="o">(</span>__<span class="o">)</span><span class="se">\ </span>      <span class="o">)</span><span class="se">\/\</span>
                <span class="o">||</span>----w <span class="p">|</span>
                <span class="o">||</span>     <span class="o">||</span>
</pre></div>
</div>
<p>Exit logs with <code class="docutils literal notranslate"><span class="pre">CTRL+C</span></code>, and the server with <code class="docutils literal notranslate"><span class="pre">exit</span></code>, and enjoy your Azure Slurm cluster.</p>
</div>
<div class="section" id="setting-up-bi-omero-in-azure-too-optional">
<h2>5. Setting up (BI)OMERO in Azure too (Optional)<a class="headerlink" href="#setting-up-bi-omero-in-azure-too-optional" title="Permalink to this headline"></a></h2>
<p>We will install (BI)OMERO on the CycleCloud VM that we have running anyway.
Alternatively, you connect your local (BI)OMERO to this cluster now.</p>
<ol class="arabic simple">
<li><p>SSH into your CycleCloud VM, <code class="docutils literal notranslate"><span class="pre">hpc-slurm-cluster</span></code> as <code class="docutils literal notranslate"><span class="pre">azureuser</span></code></p></li>
</ol>
<p><code class="docutils literal notranslate"> <span class="pre">ssh</span> <span class="pre">-i</span> <span class="pre">C:\&lt;path-to&gt;\hpc-slurm-cluster_key.pem</span> <span class="pre">azureuser&#64;&lt;public-ip&gt;</span></code></p>
<ol class="arabic simple" start="2">
<li><p><a class="reference external" href="https://www.liquidweb.com/kb/install-docker-on-linux-almalinux/">Install a container runner like Docker</a></p></li>
<li><p>Ensure it works so you can look at the lolcow again <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">run</span> <span class="pre">godlovedc/lolcow</span></code></p></li>
</ol>
<p>Ok, good enough.</p>
<p>Now let’s pull an easy BIOMERO setup from <a class="reference external" href="https://github.com/Cellular-Imaging-Amsterdam-UMC/NL-BIOMERO.git">NL-BIOMERO</a> onto our VM:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">git</span> <span class="pre">clone</span> <span class="pre">https://github.com/Cellular-Imaging-Amsterdam-UMC/NL-BIOMERO.git</span></code></p></li>
<li><p>Let’s test it: <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">compose</span> <span class="pre">up</span> <span class="pre">-d</span> <span class="pre">--build</span></code></p></li>
<li><p>Now we need to open the OMERO web port to view it <code class="docutils literal notranslate"><span class="pre">4080</span></code>.</p></li>
</ol>
<ul class="simple">
<li><p>First, go to Azure portal and click on your VM <code class="docutils literal notranslate"><span class="pre">hpc-slurm-cluster</span></code></p></li>
<li><p>Second, go to Networking &gt; Network settings</p></li>
<li><p>Third, <code class="docutils literal notranslate"><span class="pre">Create</span> <span class="pre">port</span> <span class="pre">rules</span></code> &gt; <code class="docutils literal notranslate"><span class="pre">Inbound</span> <span class="pre">port</span> <span class="pre">rule</span></code></p>
<ul>
<li><p>Destination port ranges <code class="docutils literal notranslate"><span class="pre">4080</span></code>, Protocol <code class="docutils literal notranslate"><span class="pre">TCP</span></code>, Name <code class="docutils literal notranslate"><span class="pre">OMEROWEB</span></code>. Add it. And wait a bit for it to take effect.</p></li>
</ul>
</li>
</ul>
<ol class="arabic simple" start="4">
<li><p>Test it! Open your web browser at <code class="docutils literal notranslate"><span class="pre">&lt;public-ip&gt;:4080</span></code> and login with <code class="docutils literal notranslate"><span class="pre">root</span></code>/<code class="docutils literal notranslate"><span class="pre">omero</span></code></p></li>
</ol>
<p>Good start!</p>
<p>Now let’s connect BIOMERO to our HPC Slurm cluster:</p>
<ol class="arabic simple">
<li><p>Copy the SSH private key <code class="docutils literal notranslate"><span class="pre">hpc-slurm-cluster_key.pem</span></code> (from chapter 1) to the (CycleCloud/OMERO) server:</p></li>
</ol>
<p><code class="docutils literal notranslate"><span class="pre">scp</span> <span class="pre">-c</span> <span class="pre">C:\&lt;path&gt;\hpc-slurm-cluster_key.pem</span> <span class="pre">C:\&lt;path&gt;\hpc-slurm-cluster_key.pem</span> <span class="pre">azureuser&#64;&lt;public-ip&gt;:~</span></code></p>
<ol class="arabic simple" start="2">
<li><p>Copy your key on the server into <code class="docutils literal notranslate"><span class="pre">~/.ssh</span></code> and change permissions, log in:</p></li>
</ol>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">cp</span> <span class="pre">hpc-slurm-cluster_key.pem</span> <span class="pre">.ssh</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sudo</span> <span class="pre">chmod</span> <span class="pre">700</span> <span class="pre">.ssh/hpc-slurm-cluster_key.pem</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ssh</span> <span class="pre">-i</span> <span class="pre">.ssh/hpc-slurm-cluster_key.pem</span> <span class="pre">azureadmin&#64;&lt;scheduler-ip&gt;</span></code></p></li>
<li><p>Great, exit back to the CycleCloud server.</p></li>
</ul>
<p>The IP of the scheduler (this changes whenever you create a new cluster!) is shown in the Azure CycleCloud screen, when you click on the active scheduler node.</p>
<ol class="arabic simple" start="3">
<li><p>Create a config to setup an alias for the SSH</p></li>
</ol>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">vi</span> <span class="pre">~/.ssh/config</span></code></p></li>
<li><p>press <code class="docutils literal notranslate"><span class="pre">i</span></code> to <em>insert</em> text</p></li>
<li><p>copy paste / fill in the config:</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Host</span> <span class="n">localslurm</span>
        <span class="n">HostName</span> <span class="o">&lt;</span><span class="n">scheduler</span><span class="o">-</span><span class="n">ip</span><span class="o">&gt;</span>
        <span class="n">User</span> <span class="n">azureadmin</span>
        <span class="n">Port</span> <span class="mi">22</span>
        <span class="n">IdentityFile</span> <span class="o">~/.</span><span class="n">ssh</span><span class="o">/</span><span class="n">hpc</span><span class="o">-</span><span class="n">slurm</span><span class="o">-</span><span class="n">cluster_key</span><span class="o">.</span><span class="n">pem</span>
        <span class="n">StrictHostKeyChecking</span> <span class="n">no</span>
</pre></div>
</div>
<p>Fill in the actual ip, this is just a placeholder!</p>
<ul class="simple">
<li><p>Save with escape followed by <code class="docutils literal notranslate"><span class="pre">:wq</span></code></p></li>
<li><p>chmod the config to 700 too: <code class="docutils literal notranslate"><span class="pre">sudo</span> <span class="pre">chmod</span> <span class="pre">700</span> <span class="pre">.ssh/config</span></code></p></li>
<li><p>Ready! <code class="docutils literal notranslate"><span class="pre">ssh</span> <span class="pre">localslurm</span></code> (or whatever you called the alias)</p></li>
</ul>
<ol class="arabic simple" start="4">
<li><p>Let’s edit the BIOMERO configuration <code class="docutils literal notranslate"><span class="pre">slurm-config.ini</span></code>, located in the worker-processor node</p></li>
</ol>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">vi</span> <span class="pre">~/NL-BIOMERO/worker-processor/slurm-config.ini</span></code></p></li>
<li><p>Change the <code class="docutils literal notranslate"><span class="pre">host</span></code> if you did not use the <code class="docutils literal notranslate"><span class="pre">localslurm</span></code> alias in the config above.</p></li>
<li><p>Change ALL the <code class="docutils literal notranslate"><span class="pre">[SLURM]</span></code> paths to match our new slurm setup:</p></li>
</ul>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[SLURM]</span>
<span class="c1"># -------------------------------------</span>
<span class="c1"># Slurm settings</span>
<span class="c1"># -------------------------------------</span>
<span class="c1"># General settings for where to find things on the Slurm cluster.</span>
<span class="c1"># -------------------------------------</span>
<span class="c1"># PATHS</span>
<span class="c1"># -------------------------------------</span>
<span class="c1"># The path on SLURM entrypoint for storing datafiles</span>
<span class="c1">#</span>
<span class="c1"># Note: </span>
<span class="c1"># This example is relative to the Slurm user&#39;s home dir</span>
<span class="na">slurm_data_path</span><span class="o">=</span><span class="s">data</span>
<span class="c1"># The path on SLURM entrypoint for storing container image files</span>
<span class="c1">#</span>
<span class="c1"># Note: </span>
<span class="c1"># This example is relative to the Slurm user&#39;s home dir</span>
<span class="na">slurm_images_path</span><span class="o">=</span><span class="s">singularity_images/workflows</span>
<span class="c1"># The path on SLURM entrypoint for storing the slurm job scripts</span>
<span class="c1">#</span>
<span class="c1"># Note: </span>
<span class="c1"># This example is relative to the Slurm user&#39;s home dir</span>
<span class="na">slurm_script_path</span><span class="o">=</span><span class="s">slurm-scripts</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Save the file again with escape + <code class="docutils literal notranslate"><span class="pre">:wq</span></code></p></li>
</ul>
<ol class="arabic simple" start="5">
<li><p>Now we need to do some Linux shenanigans to mount ssh properly into the container</p></li>
</ol>
<ul class="simple">
<li><p>First, create a empty .pub file that we are missing: <code class="docutils literal notranslate"><span class="pre">touch</span> <span class="pre">~/.ssh/empty.pub</span></code></p></li>
<li><p>Second, chmod the .ssh folder and its contents fully open so Docker can access it: <code class="docutils literal notranslate"><span class="pre">chmod</span> <span class="pre">-R</span> <span class="pre">777</span> <span class="pre">~/.ssh</span></code></p></li>
<li><p>Note, if you later want to SSH from commandline again (instead of letting BIOMERO do it), just change the rights back to 700 (<code class="docutils literal notranslate"><span class="pre">chmod</span> <span class="pre">-R</span> <span class="pre">700</span> <span class="pre">~/.ssh</span></code>). This is just a Linux container building temporary permission thing.</p></li>
</ul>
<ol class="arabic simple" start="6">
<li><p>Now we will (re)start / (re)build the BIOMERO servers again</p></li>
</ol>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">cd</span> <span class="pre">NL-BIOMERO</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">compose</span> <span class="pre">down</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">compose</span> <span class="pre">up</span> <span class="pre">-d</span> <span class="pre">--build</span></code></p></li>
<li><p>Now <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">logs</span> <span class="pre">-f</span> <span class="pre">nl-biomero-omeroworker-processor-1</span></code> should show some good logs leading to: <code class="docutils literal notranslate"><span class="pre">Starting</span> <span class="pre">node</span> <span class="pre">omeroworker-processor</span></code>.</p></li>
</ul>
</div>
<div class="section" id="showtime">
<h2>6. Showtime!<a class="headerlink" href="#showtime" title="Permalink to this headline"></a></h2>
<ol class="arabic simple">
<li><p>Go to your OMERO web at <code class="docutils literal notranslate"><span class="pre">http://&lt;your-VM-ip&gt;:4080/</span></code> (<code class="docutils literal notranslate"><span class="pre">root</span></code>/<code class="docutils literal notranslate"><span class="pre">omero</span></code>)</p></li>
<li><p>Let’s initialize BIOMERO: <code class="docutils literal notranslate"><span class="pre">Run</span> <span class="pre">Script</span></code> &gt; <code class="docutils literal notranslate"><span class="pre">biomero</span></code> &gt; <code class="docutils literal notranslate"><span class="pre">init</span></code> &gt; <code class="docutils literal notranslate"><span class="pre">SLURM</span> <span class="pre">Init</span> <span class="pre">environment...</span></code>; run that script</p></li>
<li><p>While we’re waiting for that to complete, let’s checkout the basic connection: <code class="docutils literal notranslate"><span class="pre">Run</span> <span class="pre">Script</span></code> &gt; <code class="docutils literal notranslate"><span class="pre">biomero</span></code> &gt; <code class="docutils literal notranslate"><span class="pre">Example</span> <span class="pre">Minimal</span> <span class="pre">Slurm</span> <span class="pre">Script...</span></code>;</p></li>
</ol>
<ul class="simple">
<li><p>Uncheck the <code class="docutils literal notranslate"><span class="pre">Run</span> <span class="pre">Python</span></code> box, as we didn’t install that</p></li>
<li><p>Check the <code class="docutils literal notranslate"><span class="pre">Check</span> <span class="pre">SLURM</span> <span class="pre">status</span></code> box</p></li>
<li><p>Check the <code class="docutils literal notranslate"><span class="pre">Check</span> <span class="pre">Queue</span></code> box</p></li>
<li><p>Run the script, you should get something like this, an empty queue:</p></li>
</ul>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>JOBID PARTITION NAME USER ST TIME NODES NODELIST<span class="o">(</span>REASON<span class="o">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>You can try some other ones, e.g. check the <code class="docutils literal notranslate"><span class="pre">Check</span> <span class="pre">Cluster</span></code> box instead:</p></li>
</ul>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>PARTITION AVAIL TIMELIMIT NODES STATE NODELIST 
dynamic up infinite <span class="m">0</span> n/a 
hpc* up infinite <span class="m">2</span> idle~ biomero-cluster-basic-hpc-<span class="o">[</span><span class="m">1</span>-2<span class="o">]</span> 
htc up infinite <span class="m">2</span> idle~ biomero-cluster-basic-htc-<span class="o">[</span><span class="m">1</span>-2<span class="o">]</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Note if you click the <code class="docutils literal notranslate"><span class="pre">i</span></code> button next to the output, you can see the output printed in a lot more detail and better formatting. Especially if you ran multiple commands at the same time.</p></li>
</ul>
<ol class="arabic simple" start="4">
<li><p>At some point, the init script will be done, or you get a <code class="docutils literal notranslate"><span class="pre">Ice.ConnectionLostException</span></code> (which means it took too long).</p></li>
</ol>
<ul class="simple">
<li><p>Let’s see what BIOMERO created! Run  <code class="docutils literal notranslate"><span class="pre">Run</span> <span class="pre">Script</span></code> &gt; <code class="docutils literal notranslate"><span class="pre">biomero</span></code> &gt; <code class="docutils literal notranslate"><span class="pre">Example</span> <span class="pre">Minimal</span> <span class="pre">Slurm</span> <span class="pre">Script...</span></code>;</p></li>
<li><p>Uncheck the <code class="docutils literal notranslate"><span class="pre">Run</span> <span class="pre">Python</span></code> box</p></li>
<li><p>Check the <code class="docutils literal notranslate"><span class="pre">Run</span> <span class="pre">Other</span> <span class="pre">Commmand</span></code> box</p></li>
<li><p>Change the Linux Command to <code class="docutils literal notranslate"><span class="pre">ls</span> <span class="pre">-la</span> <span class="pre">**/*</span></code> (we want to check all subfolders too).</p></li>
<li><p>Run it. Press the <code class="docutils literal notranslate"><span class="pre">i</span></code> button for proper formatting and scroll down to see what we made</p></li>
</ul>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="o">===</span> <span class="nv">stdout</span> <span class="o">===</span>
-rw-r--r-- <span class="m">1</span> azureadmin azureadmin <span class="m">1336</span> Mar <span class="m">21</span> <span class="m">17</span>:44 slurm-scripts/convert_job_array.sh

my-scratch/singularity_images:
total <span class="m">0</span>
drwxrwxr-x <span class="m">3</span> azureadmin azureadmin  <span class="m">24</span> Mar <span class="m">21</span> <span class="m">17</span>:31 .
drwxrwxr-x <span class="m">3</span> azureadmin azureadmin  <span class="m">32</span> Mar <span class="m">21</span> <span class="m">17</span>:31 ..
drwxrwxr-x <span class="m">2</span> azureadmin azureadmin <span class="m">117</span> Mar <span class="m">21</span> <span class="m">17</span>:45 converters

singularity_images/workflows:
total <span class="m">16</span>
drwxrwxr-x <span class="m">6</span> azureadmin azureadmin   <span class="m">126</span> Mar <span class="m">21</span> <span class="m">17</span>:31 .
drwxrwxr-x <span class="m">3</span> azureadmin azureadmin    <span class="m">23</span> Mar <span class="m">21</span> <span class="m">17</span>:31 ..
drwxrwxr-x <span class="m">2</span> azureadmin azureadmin    <span class="m">40</span> Mar <span class="m">21</span> <span class="m">17</span>:42 cellexpansion
drwxrwxr-x <span class="m">2</span> azureadmin azureadmin    <span class="m">54</span> Mar <span class="m">21</span> <span class="m">17</span>:36 cellpose
drwxrwxr-x <span class="m">2</span> azureadmin azureadmin    <span class="m">52</span> Mar <span class="m">21</span> <span class="m">17</span>:40 cellprofiler_spot
-rw-rw-r-- <span class="m">1</span> azureadmin azureadmin   <span class="m">695</span> Mar <span class="m">21</span> <span class="m">17</span>:45 pull_images.sh
-rw-rw-r-- <span class="m">1</span> azureadmin azureadmin <span class="m">10802</span> Mar <span class="m">21</span> <span class="m">17</span>:45 sing.log
drwxrwxr-x <span class="m">2</span> azureadmin azureadmin    <span class="m">43</span> Mar <span class="m">21</span> <span class="m">17</span>:44 spotcounting

slurm-scripts/jobs:
total <span class="m">16</span>
drwxrwxr-x <span class="m">2</span> azureadmin azureadmin  <span class="m">100</span> Mar <span class="m">21</span> <span class="m">17</span>:31 .
drwxrwxr-x <span class="m">3</span> azureadmin azureadmin   <span class="m">46</span> Mar <span class="m">21</span> <span class="m">17</span>:31 ..
-rw-rw-r-- <span class="m">1</span> azureadmin azureadmin <span class="m">3358</span> Mar <span class="m">21</span> <span class="m">17</span>:44 cellexpansion.sh
-rw-rw-r-- <span class="m">1</span> azureadmin azureadmin <span class="m">3406</span> Mar <span class="m">21</span> <span class="m">17</span>:44 cellpose.sh
-rw-rw-r-- <span class="m">1</span> azureadmin azureadmin <span class="m">3184</span> Mar <span class="m">21</span> <span class="m">17</span>:44 cellprofiler_spot.sh
-rw-rw-r-- <span class="m">1</span> azureadmin azureadmin <span class="m">3500</span> Mar <span class="m">21</span> <span class="m">17</span>:44 spotcounting.sh
</pre></div>
</div>
<ul class="simple">
<li><p>Or better yet, run this linux command for full info on <em>all</em> (non-hidden) subdirectories: <code class="docutils literal notranslate"><span class="pre">find</span> <span class="pre">.</span> <span class="pre">-type</span> <span class="pre">d</span> <span class="pre">-not</span> <span class="pre">-path</span> <span class="pre">'*/.*'</span> <span class="pre">-exec</span> <span class="pre">ls</span> <span class="pre">-la</span> <span class="pre">{}</span> <span class="pre">+</span></code>. This should show that we downloaded some of the workflows to our Slurm cluster already:</p></li>
</ul>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>./singularity_images/workflows:
total <span class="m">16</span>
drwxrwxr-x <span class="m">6</span> azureadmin azureadmin   <span class="m">126</span> Mar <span class="m">21</span> <span class="m">17</span>:31 .
drwxrwxr-x <span class="m">3</span> azureadmin azureadmin    <span class="m">23</span> Mar <span class="m">21</span> <span class="m">17</span>:31 ..
drwxrwxr-x <span class="m">2</span> azureadmin azureadmin    <span class="m">40</span> Mar <span class="m">21</span> <span class="m">17</span>:42 cellexpansion
drwxrwxr-x <span class="m">2</span> azureadmin azureadmin    <span class="m">54</span> Mar <span class="m">21</span> <span class="m">17</span>:36 cellpose
drwxrwxr-x <span class="m">2</span> azureadmin azureadmin    <span class="m">52</span> Mar <span class="m">21</span> <span class="m">17</span>:40 cellprofiler_spot
-rw-rw-r-- <span class="m">1</span> azureadmin azureadmin   <span class="m">695</span> Mar <span class="m">21</span> <span class="m">17</span>:45 pull_images.sh
-rw-rw-r-- <span class="m">1</span> azureadmin azureadmin <span class="m">10802</span> Mar <span class="m">21</span> <span class="m">17</span>:45 sing.log
drwxrwxr-x <span class="m">2</span> azureadmin azureadmin    <span class="m">43</span> Mar <span class="m">21</span> <span class="m">17</span>:44 spotcounting

./singularity_images/workflows/cellexpansion:
total <span class="m">982536</span>
drwxrwxr-x <span class="m">2</span> azureadmin azureadmin         <span class="m">40</span> Mar <span class="m">21</span> <span class="m">17</span>:42 .
drwxrwxr-x <span class="m">6</span> azureadmin azureadmin        <span class="m">126</span> Mar <span class="m">21</span> <span class="m">17</span>:31 ..
-rwxr-xr-x <span class="m">1</span> azureadmin azureadmin <span class="m">1006116864</span> Mar <span class="m">21</span> <span class="m">17</span>:42 w_cellexpansion_v2.0.1.sif

./singularity_images/workflows/cellpose:
total <span class="m">4672820</span>
drwxrwxr-x <span class="m">2</span> azureadmin azureadmin         <span class="m">54</span> Mar <span class="m">21</span> <span class="m">17</span>:36 .
drwxrwxr-x <span class="m">6</span> azureadmin azureadmin        <span class="m">126</span> Mar <span class="m">21</span> <span class="m">17</span>:31 ..
-rwxr-xr-x <span class="m">1</span> azureadmin azureadmin <span class="m">4784967680</span> Mar <span class="m">21</span> <span class="m">17</span>:36 t_nucleisegmentation-cellpose_v1.2.9.sif

./singularity_images/workflows/cellprofiler_spot:
total <span class="m">2215916</span>
drwxrwxr-x <span class="m">2</span> azureadmin azureadmin         <span class="m">52</span> Mar <span class="m">21</span> <span class="m">17</span>:40 .
drwxrwxr-x <span class="m">6</span> azureadmin azureadmin        <span class="m">126</span> Mar <span class="m">21</span> <span class="m">17</span>:31 ..
-rwxr-xr-x <span class="m">1</span> azureadmin azureadmin <span class="m">2269097984</span> Mar <span class="m">21</span> <span class="m">17</span>:40 w_spotcounting-cellprofiler_v1.0.1.sif

./singularity_images/workflows/spotcounting:
total <span class="m">982720</span>
drwxrwxr-x <span class="m">2</span> azureadmin azureadmin         <span class="m">43</span> Mar <span class="m">21</span> <span class="m">17</span>:44 .
drwxrwxr-x <span class="m">6</span> azureadmin azureadmin        <span class="m">126</span> Mar <span class="m">21</span> <span class="m">17</span>:31 ..
-rwxr-xr-x <span class="m">1</span> azureadmin azureadmin <span class="m">1006305280</span> Mar <span class="m">21</span> <span class="m">17</span>:44 w_countmaskoverlap_v1.0.1.sif

./slurm-scripts:
total <span class="m">8</span>
drwxrwxr-x  <span class="m">3</span> azureadmin azureadmin   <span class="m">46</span> Mar <span class="m">21</span> <span class="m">17</span>:31 .
drwxr-xr-x <span class="m">12</span> azureadmin azureadmin <span class="m">4096</span> Mar <span class="m">21</span> <span class="m">17</span>:31 ..
-rw-r--r--  <span class="m">1</span> azureadmin azureadmin <span class="m">1336</span> Mar <span class="m">21</span> <span class="m">17</span>:44 convert_job_array.sh
drwxrwxr-x  <span class="m">2</span> azureadmin azureadmin  <span class="m">100</span> Mar <span class="m">21</span> <span class="m">17</span>:31 <span class="nb">jobs</span>

./slurm-scripts/jobs:
total <span class="m">16</span>
drwxrwxr-x <span class="m">2</span> azureadmin azureadmin  <span class="m">100</span> Mar <span class="m">21</span> <span class="m">17</span>:31 .
drwxrwxr-x <span class="m">3</span> azureadmin azureadmin   <span class="m">46</span> Mar <span class="m">21</span> <span class="m">17</span>:31 ..
-rw-rw-r-- <span class="m">1</span> azureadmin azureadmin <span class="m">3358</span> Mar <span class="m">21</span> <span class="m">17</span>:44 cellexpansion.sh
-rw-rw-r-- <span class="m">1</span> azureadmin azureadmin <span class="m">3406</span> Mar <span class="m">21</span> <span class="m">17</span>:44 cellpose.sh
-rw-rw-r-- <span class="m">1</span> azureadmin azureadmin <span class="m">3184</span> Mar <span class="m">21</span> <span class="m">17</span>:44 cellprofiler_spot.sh
-rw-rw-r-- <span class="m">1</span> azureadmin azureadmin <span class="m">3500</span> Mar <span class="m">21</span> <span class="m">17</span>:44 spotcounting.sh
</pre></div>
</div>
<ol class="arabic simple" start="5">
<li><p>Ok, let’s get to some data! Upload a file with (a local installation of) omero insight.</p></li>
</ol>
<ul class="simple">
<li><p>First, open up the OMERO port <code class="docutils literal notranslate"><span class="pre">4064</span></code> in Azure on your <code class="docutils literal notranslate"><span class="pre">hpc-slurm-cluster</span></code>, just like we did with port <code class="docutils literal notranslate"><span class="pre">4080</span></code>: Add inbound security rule, destination <code class="docutils literal notranslate"><span class="pre">4064</span></code>, Protocol <code class="docutils literal notranslate"><span class="pre">TCP</span></code>, Name <code class="docutils literal notranslate"><span class="pre">OMEROINSIGHT</span></code>.</p></li>
<li><p>Change the server to <code class="docutils literal notranslate"><span class="pre">&lt;cyclecloud-vm-ip&gt;:4064</span></code></p></li>
<li><p>Login <code class="docutils literal notranslate"><span class="pre">root</span></code>/<code class="docutils literal notranslate"><span class="pre">omero</span></code></p></li>
<li><p>Upload some Nuclei fluorescense images. For example, I uploaded the raw images from <a class="reference external" href="https://www.ebi.ac.uk/biostudies/bioimages/studies/S-BSST265">S-BSST265</a> into a Project <code class="docutils literal notranslate"><span class="pre">TestProject</span></code> and Dataset <code class="docutils literal notranslate"><span class="pre">S-BSST265</span></code>. Add to Queue, and import!</p></li>
</ul>
<ol class="arabic simple" start="6">
<li><p>IMPORTANT! Our default <a class="reference external" href="https://github.com/NL-BioImaging/biomero/blob/main/resources/job_template.sh#L12">job script</a> assumes 4 CPUs, but we have nodes with only 2 cores. So we have to lower this amount for the job script. Otherwise we get this error:</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sbatch</span><span class="p">:</span> <span class="n">error</span><span class="p">:</span> <span class="n">CPU</span> <span class="n">count</span> <span class="n">per</span> <span class="n">node</span> <span class="n">can</span> <span class="ow">not</span> <span class="n">be</span> <span class="n">satisfied</span>
<span class="n">sbatch</span><span class="p">:</span> <span class="n">error</span><span class="p">:</span> <span class="n">Batch</span> <span class="n">job</span> <span class="n">submission</span> <span class="n">failed</span><span class="p">:</span> <span class="n">Requested</span> <span class="n">node</span> <span class="n">configuration</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">available</span>
</pre></div>
</div>
<p>We will do this ad-hoc, by changing the configuration for CellPose in the <code class="docutils literal notranslate"><span class="pre">slurm-config.ini</span></code> in our installation:</p>
<ul class="simple">
<li><p>First, edit the config on the main VM with <code class="docutils literal notranslate"><span class="pre">vi</span> <span class="pre">worker-processor/slurm-config.ini</span></code></p></li>
<li><p>Add this line to your workflows <code class="docutils literal notranslate"><span class="pre">&lt;wf&gt;_job_cpus-per-task=2</span></code>, e.g. <code class="docutils literal notranslate"><span class="pre">cellpose_job_cpus-per-task=2</span></code></p></li>
<li><p>save file (<code class="docutils literal notranslate"><span class="pre">:wq</span></code>)</p></li>
<li><p>Don’t forget to open your .ssh to the container <code class="docutils literal notranslate"><span class="pre">chmod</span> <span class="pre">-R</span> <span class="pre">777</span> <span class="pre">~/.ssh</span></code> (and close it later)</p></li>
<li><p>Restart the biomero container(s) (<code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">compose</span> <span class="pre">down</span></code> &amp; <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">compose</span> <span class="pre">up</span> <span class="pre">-d</span> <span class="pre">--build</span></code>, perhaps specifically for <code class="docutils literal notranslate"><span class="pre">omeroworker-processor</span></code>).</p></li>
<li><p>Check logs to see if biomero started up properly <code class="docutils literal notranslate"><span class="pre">docker</span> <span class="pre">logs</span> <span class="pre">-f</span> <span class="pre">nl-biomero-omeroworker-processor-1</span></code></p></li>
</ul>
<ol class="arabic simple" start="6">
<li><p>Next, time to segment! Time to spin up those SLURM compute nodes:</p></li>
</ol>
<ul class="simple">
<li><p>First, select your newly imported dataset, then <code class="docutils literal notranslate"><span class="pre">Run</span> <span class="pre">Script</span></code> &gt; <code class="docutils literal notranslate"><span class="pre">biomero</span></code> &gt; <code class="docutils literal notranslate"><span class="pre">workflows</span></code> &gt; <code class="docutils literal notranslate"><span class="pre">SLURM</span> <span class="pre">Run</span> <span class="pre">Workflow...</span></code></p></li>
<li><p>At <code class="docutils literal notranslate"><span class="pre">Select</span> <span class="pre">how</span> <span class="pre">to</span> <span class="pre">import</span> <span class="pre">your</span> <span class="pre">results</span> <span class="pre">(one</span> <span class="pre">or</span> <span class="pre">more)</span></code>, we will upload the masks back into a new dataset, so:</p>
<ul>
<li><p>Change <code class="docutils literal notranslate"><span class="pre">3a)</span> <span class="pre">Import</span> <span class="pre">into</span> <span class="pre">NEW</span> <span class="pre">Dataset:</span></code> into <code class="docutils literal notranslate"><span class="pre">CellPoseMasks</span></code></p></li>
<li><p>Change <code class="docutils literal notranslate"><span class="pre">3c)</span> <span class="pre">Rename</span> <span class="pre">the</span> <span class="pre">imported</span> <span class="pre">images:</span></code> into <code class="docutils literal notranslate"><span class="pre">{original_file}_Mask_C1.{ext}</span></code> (these are placeholder values)</p></li>
</ul>
</li>
<li><p>Next, check the <code class="docutils literal notranslate"><span class="pre">cellpose</span></code> box and</p>
<ul>
<li><p>Change <code class="docutils literal notranslate"><span class="pre">nuc</span> <span class="pre">channel</span></code> to <code class="docutils literal notranslate"><span class="pre">1</span></code></p></li>
<li><p>Uncheck the <code class="docutils literal notranslate"><span class="pre">use</span> <span class="pre">gpu</span></code> box (unless you paid for sweet GPU nodes from Azure)</p></li>
</ul>
</li>
<li><p>Run Script!</p>
<ul>
<li><p>We are running the <code class="docutils literal notranslate"><span class="pre">cellpose</span></code> workflow on channel <code class="docutils literal notranslate"><span class="pre">1</span></code> (with otherwise default parameters) of all the images of the dataset <code class="docutils literal notranslate"><span class="pre">S-BSST265</span></code> and import the output mask images back into OMERO as dataset <code class="docutils literal notranslate"><span class="pre">CellPoseMasks</span></code>.</p></li>
</ul>
</li>
<li><p>Now, this will take a while again because we are cheap and do not have a GPU node at the ready.</p>
<ul>
<li><p>Instead, Azure will <code class="docutils literal notranslate"><span class="pre">on-demand</span></code> create our compute node (to save us money when we are not using it), which only has a few CPUs as well!</p></li>
<li><p>So this is not a test of speed (unless you setup a nice Slurm cluster with always-available GPU nodes), but of the BIOMERO automation process.</p></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="extra-thoughts">
<h1>Extra thoughts<a class="headerlink" href="#extra-thoughts" title="Permalink to this headline"></a></h1>
<ul class="simple">
<li><p>Perhaps also make the Cluster have a static IP, instead of changing whenever you terminate it: https://learn.microsoft.com/en-us/azure/cyclecloud/how-to/network-security?view=cyclecloud-8</p></li>
</ul>
</div>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="readme_link.html" class="btn btn-neutral float-left" title="BIOMERO - BioImage analysis in OMERO" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="modules.html" class="btn btn-neutral float-right" title="biomero" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2023, T.T.Luik.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>